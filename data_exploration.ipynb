{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open and explore data from:\n",
    "    https://figshare.com/articles/1000homes/1270900\n",
    "    \n",
    "To open Biom-format files:\n",
    "    $ pip install biom-format\n",
    "    \n",
    "For more on Biom-format files:\n",
    "    http://biom-format.org\n",
    "    https://github.com/biocore/biom-format/issues/622\n",
    "    \n",
    "To do:\n",
    "    Avoid using append. Try using merge instead to join dataframes\n",
    "    http://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "    http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir('data/otu_tables_wTax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def exploding_panda(_bt):\n",
    "    \"\"\"BIOM->Pandas dataframe converter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _bt : biom.Table\n",
    "        BIOM table\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The BIOM table converted into a DataFrame\n",
    "        object.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Based on this answer on SO:\n",
    "    http://stackoverflow.com/a/17819427/379593\n",
    "    \"\"\"\n",
    "    m = _bt.matrix_data\n",
    "    data = [pd.SparseSeries(m[i].toarray().ravel()) for i in np.arange(m.shape[0])]\n",
    "    out = pd.SparseDataFrame(data, index=_bt.ids('observation'),\n",
    "                             columns=_bt.ids('sample'))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import biom\n",
    "from biom import parse_table, load_table\n",
    "fname = 'ITS_otu_table_wTax.biom'\n",
    "with open(fname) as f: \n",
    "    table = parse_table(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74851 x 2688 <class 'biom.table.Table'> with 4430534 nonzero entries (2% dense)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Constructed from biom file\n",
      "#OTU ID\t242.O\t919.O\t330.O\t445.O\t733.O\n",
      "OTU_360\t30963.0\t2.0\t1.0\t19.0\t0.0\n",
      "OTU_5\t2.0\t53536.0\t6.0\t36.0\t759.0\n",
      "OTU_47557\t0.0\t0.0\t1.0\t0.0\t0.0\n",
      "OTU_45150\t69.0\t23.0\t202.0\t1963.0\t9.0\n",
      "OTU_3\t365.0\t9387.0\t671.0\t95.0\t29932.0\n"
     ]
    }
   ],
   "source": [
    "print table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'OTU_360' u'OTU_5' u'OTU_47557' ..., u'OTU_55963' u'OTU_47187'\n",
      " u'OTU_70298']\n"
     ]
    }
   ],
   "source": [
    "# print ids along observation axis\n",
    "print table.ids(axis='observation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'242.O' u'919.O' u'330.O' ..., u'1459.O' u'1454.O' u'1267.O']\n"
     ]
    }
   ],
   "source": [
    "# print ids along the sample axis\n",
    "print table.ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# to return the index of the identified sample/observation\n",
    "print table.index('OTU_5','observation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 305 2371   32 ...,    1    1    1]\n"
     ]
    }
   ],
   "source": [
    "# find out how many nonzero elts there are; returns a numpy array\n",
    "print table.nonzero_counts('observation',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74851 x 2688 <class 'biom.table.Table'> with 4430534 nonzero entries (2% dense)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty samples or observations from the table\n",
    "table_new = table.remove_empty(axis='whole',inplace=False)\n",
    "table_new # looks like nothing was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Constructed from biom file\n",
      "#OTU ID\t762.O\t935.O\t956.O\t309.I\t557.I\n",
      "OTU_5\t1742.0\t3.0\t1.0\t1439.0\t20.0\n",
      "OTU_47557\t0.0\t0.0\t0.0\t0.0\t0.0\n",
      "OTU_45150\t450.0\t1.0\t0.0\t0.0\t252.0\n",
      "OTU_3\t7.0\t3.0\t9.0\t0.0\t288.0\n",
      "OTU_29537\t21.0\t0.0\t2.0\t1.0\t2.0\n"
     ]
    }
   ],
   "source": [
    "# randomly subsample without replacement\n",
    "num_sample = 10\n",
    "samp_table = table.subsample(num_sample,axis='sample',by_id=True)\n",
    "print samp_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>762.O</th>\n",
       "      <th>935.O</th>\n",
       "      <th>956.O</th>\n",
       "      <th>309.I</th>\n",
       "      <th>557.I</th>\n",
       "      <th>318.I</th>\n",
       "      <th>250.I</th>\n",
       "      <th>1438.I</th>\n",
       "      <th>1332.I</th>\n",
       "      <th>1408.O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OTU_5</th>\n",
       "      <td>1742.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_47557</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_45150</th>\n",
       "      <td>450.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>10542.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_29537</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>367.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            762.O  935.O  956.O   309.I  557.I    318.I  250.I  1438.I  \\\n",
       "OTU_5      1742.0    3.0    1.0  1439.0   20.0    140.0    4.0     0.0   \n",
       "OTU_47557     0.0    0.0    0.0     0.0    0.0      0.0    0.0     0.0   \n",
       "OTU_45150   450.0    1.0    0.0     0.0  252.0      1.0    0.0     0.0   \n",
       "OTU_3         7.0    3.0    9.0     0.0  288.0  10542.0  171.0     7.0   \n",
       "OTU_29537    21.0    0.0    2.0     1.0    2.0    600.0    0.0     1.0   \n",
       "\n",
       "           1332.I  1408.O  \n",
       "OTU_5        75.0     3.0  \n",
       "OTU_47557     1.0     0.0  \n",
       "OTU_45150     0.0     0.0  \n",
       "OTU_3       175.0   334.0  \n",
       "OTU_29537    24.0   367.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert matrix data to a Pandas SparseDataFrame, indexed on\n",
    "# observation IDs, with the column names as the sample IDs\n",
    "# excludes metadata\n",
    "\n",
    "# note: if done on the full table, the notebook crashes. \n",
    "# try converting the random subsample\n",
    "df = samp_table.to_dataframe() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method iter in module biom.table:\n",
      "\n",
      "iter(self, dense=True, axis='sample') method of biom.table.Table instance\n",
      "    Yields ``(value, id, metadata)``\n",
      "    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    dense : bool, optional\n",
      "        Defaults to ``True``. If ``False``, yield compressed sparse row or\n",
      "        compressed sparse columns if `axis` is 'observation' or 'sample',\n",
      "        respectively.\n",
      "    axis : {'sample', 'observation'}, optional\n",
      "        The axis to iterate over.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    GeneratorType\n",
      "        A generator that yields (values, id, metadata)\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from biom.table import Table\n",
      "    \n",
      "    Create a 2x3 BIOM table:\n",
      "    \n",
      "    >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      "    >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'Z3'])\n",
      "    \n",
      "    Iter over samples and keep those that start with an Z:\n",
      "    \n",
      "    >>> [(values, id, metadata)\n",
      "    ...     for values, id, metadata in table.iter() if id[0]=='Z']\n",
      "    [(array([  1.,  42.]), 'Z3', None)]\n",
      "    \n",
      "    Iter over observations and add the 2nd column of the values\n",
      "    \n",
      "    >>> col = [values[1] for values, id, metadata in table.iter()]\n",
      "    >>> sum(col)\n",
      "    46.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(table.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Table in module biom.table object:\n",
      "\n",
      "class Table(__builtin__.object)\n",
      " |  The (canonically pronounced 'teh') Table.\n",
      " |  \n",
      " |  Give in to the power of the Table!\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Equality is determined by the data matrix, metadata, and IDs\n",
      " |  \n",
      " |  __getitem__(self, args)\n",
      " |      Handles row or column slices\n",
      " |      \n",
      " |      Slicing over an individual axis is supported, but slicing over both\n",
      " |      axes at the same time is not supported. Partial slices, such as\n",
      " |      `foo[0, 5:10]` are not supported, however full slices are supported,\n",
      " |      such as `foo[0, :]`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : tuple or slice\n",
      " |          The specific element (by index position) to return or an entire\n",
      " |          row or column of the data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float or spmatrix\n",
      " |          A float is return if a specific element is specified, otherwise a\n",
      " |          spmatrix object representing a vector of sparse data is returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IndexError\n",
      " |          - If the matrix is empty\n",
      " |          - If the arguments do not appear to be a tuple\n",
      " |          - If a slice on row and column is specified\n",
      " |          - If a partial slice is specified\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Switching between slicing rows and columns is inefficient.  Slicing of\n",
      " |      rows requires a CSR representation, while slicing of columns requires a\n",
      " |      CSC representation, and transforms are performed on the data if the\n",
      " |      data are not in the required representation. These transforms can be\n",
      " |      expensive if done frequently.\n",
      " |      \n",
      " |      .. shownumpydoc\n",
      " |  \n",
      " |  __init__(self, data, observation_ids, sample_ids, observation_metadata=None, sample_metadata=None, table_id=None, type=None, create_date=None, generated_by=None, observation_group_metadata=None, sample_group_metadata=None, **kwargs)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      See ``biom.table.Table.iter``\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns a high-level summary of the table's properties\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          A string detailing the shape, class, number of nonzero entries, and\n",
      " |          table density\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Stringify self\n",
      " |      \n",
      " |      Default str output for a Table is just row/col ids and data values\n",
      " |  \n",
      " |  add_group_metadata(self, group_md, axis='sample')\n",
      " |      Take a dict of group metadata and add it to an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group_md : dict of tuples\n",
      " |          `group_md` should be of the form ``{category: (data type, value)``\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |  \n",
      " |  add_metadata(self, md, axis='sample')\n",
      " |      Take a dict of metadata and add it to an axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      md : dict of dict\n",
      " |          `md` should be of the form ``{id: {dict_of_metadata}}``\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on\n",
      " |  \n",
      " |  collapse(self, f, collapse_f=None, norm=True, min_group_size=1, include_collapsed_metadata=True, one_to_many=False, one_to_many_mode='add', one_to_many_md_key='Path', strict=False, axis='sample')\n",
      " |      Collapse partitions in a table by metadata or by IDs\n",
      " |      \n",
      " |      Partition data by metadata or IDs and then collapse each partition into\n",
      " |      a single vector.\n",
      " |      \n",
      " |      If `include_collapsed_metadata` is ``True``, the metadata for the\n",
      " |      collapsed partition will be a category named 'collapsed_ids', in which\n",
      " |      a list of the original ids that made up the partition is retained\n",
      " |      \n",
      " |      The remainder is only relevant to setting `one_to_many` to ``True``.\n",
      " |      \n",
      " |      If `one_to_many` is ``True``, allow vectors to collapse into multiple\n",
      " |      bins if the metadata describe a one-many relationship. Supplied\n",
      " |      functions must allow for iteration support over the metadata key and\n",
      " |      must return a tuple of (path, bin) as to describe both the path in the\n",
      " |      hierarchy represented and the specific bin being collapsed into. The\n",
      " |      uniqueness of the bin is _not_ based on the path but by the name of the\n",
      " |      bin.\n",
      " |      \n",
      " |      The metadata value for the corresponding collapsed column may include\n",
      " |      more (or less) information about the collapsed data. For example, if\n",
      " |      collapsing \"FOO\", and there are vectors that span three associations A,\n",
      " |      B, and C, such that vector 1 spans A and B, vector 2 spans B and C and\n",
      " |      vector 3 spans A and C, the resulting table will contain three\n",
      " |      collapsed vectors:\n",
      " |      \n",
      " |      - A, containing original vectors 1 and 3\n",
      " |      - B, containing original vectors 1 and 2\n",
      " |      - C, containing original vectors 2 and 3\n",
      " |      \n",
      " |      If a vector maps to the same partition multiple times, it will be\n",
      " |      counted multiple times.\n",
      " |      \n",
      " |      There are two supported modes for handling one-to-many relationships\n",
      " |      via `one_to_many_mode`: ``add`` and `divide`. ``add`` will add the\n",
      " |      vector counts to each partition that the vector maps to, which may\n",
      " |      increase the total number of counts in the output table. ``divide``\n",
      " |      will divide a vectors's counts by the number of metadata that the\n",
      " |      vector has before adding the counts to each partition. This will not\n",
      " |      increase the total number of counts in the output table.\n",
      " |      \n",
      " |      If `one_to_many_md_key` is specified, that becomes the metadata\n",
      " |      key that describes the collapsed path. If a value is not specified,\n",
      " |      then it defaults to 'Path'.\n",
      " |      \n",
      " |      If `strict` is specified, then all metadata pathways operated on\n",
      " |      must be indexable by `metadata_f`.\n",
      " |      \n",
      " |      `one_to_many` and `norm` are not supported together.\n",
      " |      \n",
      " |      `one_to_many` and `collapse_f` are not supported together.\n",
      " |      \n",
      " |      `one_to_many` and `min_group_size` are not supported together.\n",
      " |      \n",
      " |      A final note on space consumption. At present, the `one_to_many`\n",
      " |      functionality requires a temporary dense matrix representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          Function that is used to determine what partition a vector belongs\n",
      " |          to\n",
      " |      collapse_f : function, optional\n",
      " |          Function that collapses a partition in a one-to-one collapse. The\n",
      " |          expected function signature is:\n",
      " |      \n",
      " |              dense or sparse_vector <- collapse_f(Table, axis)\n",
      " |      \n",
      " |          Defaults to a pairwise add.\n",
      " |      \n",
      " |      norm : bool, optional\n",
      " |          Defaults to ``True``. If ``True``, normalize the resulting table\n",
      " |      min_group_size : int, optional\n",
      " |          Defaults to ``1``. The minimum size of a partition when performing\n",
      " |          a one-to-one collapse\n",
      " |      include_collapsed_metadata : bool, optional\n",
      " |          Defaults to ``True``. If ``True``, retain the collapsed metadata\n",
      " |          keyed by the original IDs of the associated vectors\n",
      " |      one_to_many : bool, optional\n",
      " |          Defaults to ``False``. Perform a one-to-many collapse\n",
      " |      one_to_many_mode : {'add', 'divide'}, optional\n",
      " |          The way to reduce two vectors in a one-to-many collapse\n",
      " |      one_to_many_md_key : str, optional\n",
      " |          Defaults to \"Path\". If `include_collapsed_metadata` is ``True``,\n",
      " |          store the original vector metadata under this key\n",
      " |      strict : bool, optional\n",
      " |          Defaults to ``False``. Requires full pathway data within a\n",
      " |          one-to-many structure\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to collapse\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          The collapsed table\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a ``Table``\n",
      " |      \n",
      " |      >>> dt_rich = Table(\n",
      " |      ...    np.array([[5, 6, 7], [8, 9, 10], [11, 12, 13]]),\n",
      " |      ...    ['1', '2', '3'], ['a', 'b', 'c'],\n",
      " |      ...    [{'taxonomy': ['k__a', 'p__b']},\n",
      " |      ...     {'taxonomy': ['k__a', 'p__c']},\n",
      " |      ...     {'taxonomy': ['k__a', 'p__c']}],\n",
      " |      ...    [{'barcode': 'aatt'},\n",
      " |      ...     {'barcode': 'ttgg'},\n",
      " |      ...     {'barcode': 'aatt'}])\n",
      " |      >>> print dt_rich # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID a   b   c\n",
      " |      1   5.0 6.0 7.0\n",
      " |      2   8.0 9.0 10.0\n",
      " |      3   11.0    12.0    13.0\n",
      " |      \n",
      " |      Create Function to determine what partition a vector belongs to\n",
      " |      \n",
      " |      >>> bin_f = lambda id_, x: x['taxonomy'][1]\n",
      " |      >>> obs_phy = dt_rich.collapse(\n",
      " |      ...    bin_f, norm=False, min_group_size=1,\n",
      " |      ...    axis='observation').sort(axis='observation')\n",
      " |      >>> print obs_phy # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID a   b   c\n",
      " |      p__b    5.0 6.0 7.0\n",
      " |      p__c    19.0    21.0    23.0\n",
      " |  \n",
      " |  concat(self, others, axis='sample')\n",
      " |      Concatenate tables if axis is disjoint\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      others : iterable of biom.Table\n",
      " |          Tables to concatenate\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to concatenate on. i.e., if axis is 'sample', then tables\n",
      " |          will be joined such that the set of sample IDs in the resulting\n",
      " |          table will be the union of sample IDs across all tables in others.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      DisjointIDError\n",
      " |          If IDs over the axis are not disjoint.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The type of the table is inherited from self.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Concatenate three tables in which the sample IDs are disjoint. Note\n",
      " |      the observation IDs in this example are not disjoint (although they\n",
      " |      can be):\n",
      " |      \n",
      " |      >>> from biom import Table\n",
      " |      >>> import numpy as np\n",
      " |      >>> a = Table(np.array([[0, 1, 2], [3, 4, 5]]), ['O1', 'O2'],\n",
      " |      ...                     ['S1', 'S2', 'S3'],\n",
      " |      ...                     [{'taxonomy': 'foo'}, {'taxonomy': 'bar'}])\n",
      " |      >>> b = Table(np.array([[6, 7, 8], [9, 10, 11]]), ['O3', 'O4'],\n",
      " |      ...                     ['S4', 'S5', 'S6'],\n",
      " |      ...                     [{'taxonomy': 'baz'}, {'taxonomy': 'foobar'}])\n",
      " |      >>> c = Table(np.array([[12, 13, 14], [15, 16, 17]]), ['O1', 'O5'],\n",
      " |      ...                     ['S7', 'S8', 'S9'],\n",
      " |      ...                     [{'taxonomy': 'foo'}, {'taxonomy': 'biz'}])\n",
      " |      >>> d = a.concat([b, c])\n",
      " |      >>> print(d)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1      S2      S3      S4      S5      S6      S7      S8      S9\n",
      " |      O1      0.0     1.0     2.0     0.0     0.0     0.0     12.0    13.0    14.0\n",
      " |      O2      3.0     4.0     5.0     0.0     0.0     0.0     0.0     0.0     0.0\n",
      " |      O3      0.0     0.0     0.0     6.0     7.0     8.0     0.0     0.0     0.0\n",
      " |      O4      0.0     0.0     0.0     9.0     10.0    11.0    0.0     0.0     0.0\n",
      " |      O5      0.0     0.0     0.0     0.0     0.0     0.0     15.0    16.0    17.0\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Returns a copy of the table\n",
      " |  \n",
      " |  data(self, id, axis='sample', dense=True)\n",
      " |      Returns data associated with an `id`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id : str\n",
      " |          ID of the sample or observation whose data will be returned.\n",
      " |      axis : {'sample', 'observation'}\n",
      " |          Axis to search for `id`.\n",
      " |      dense : bool, optional\n",
      " |          If ``True``, return data as dense\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray or scipy.sparse.spmatrix\n",
      " |          np.ndarray if ``dense``, otherwise scipy.sparse.spmatrix\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> example_table.data('S1', axis='sample')\n",
      " |      array([ 0.,  3.])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Table.get_value_by_ids\n",
      " |  \n",
      " |  del_metadata(self, keys=None, axis='whole')\n",
      " |      Remove metadata from an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keys : list of str, optional\n",
      " |          The keys to remove from metadata. If None, all keys from the axis\n",
      " |          are removed.\n",
      " |      axis : {'sample', 'observation', 'whole'}, optional\n",
      " |          The axis to operate on. If 'whole', the operation is applied to\n",
      " |          both the sample and observation axes.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If the requested axis does not exist.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import Table\n",
      " |      >>> import numpy as np\n",
      " |      >>> tab = Table(np.array([[1, 2], [3, 4]]),\n",
      " |      ...             ['O1', 'O2'],\n",
      " |      ...             ['S1', 'S2'],\n",
      " |      ...             sample_metadata=[{'barcode': 'ATGC', 'env': 'A'},\n",
      " |      ...                              {'barcode': 'GGTT', 'env': 'B'}])\n",
      " |      >>> tab.del_metadata(keys=['env'])\n",
      " |      >>> for id, md in zip(tab.ids(), tab.metadata()):\n",
      " |      ...     print(id, list(md.items()))\n",
      " |      ('S1', [('barcode', 'ATGC')])\n",
      " |      ('S2', [('barcode', 'GGTT')])\n",
      " |  \n",
      " |  delimited_self(self, delim=u'\\t', header_key=None, header_value=None, metadata_formatter=<type 'str'>, observation_column_name=u'#OTU ID')\n",
      " |      Return self as a string in a delimited form\n",
      " |      \n",
      " |      Default str output for the Table is just row/col ids and table data\n",
      " |      without any metadata\n",
      " |      \n",
      " |      Including observation metadata in output: If ``header_key`` is not\n",
      " |      ``None``, the observation metadata with that name will be included\n",
      " |      in the delimited output. If ``header_value`` is also not ``None``, the\n",
      " |      observation metadata will use the provided ``header_value`` as the\n",
      " |      observation metadata name (i.e., the column header) in the delimited\n",
      " |      output.\n",
      " |      \n",
      " |      ``metadata_formatter``: a function which takes a metadata entry and\n",
      " |      returns a formatted version that should be written to file\n",
      " |      \n",
      " |      ``observation_column_name``: the name of the first column in the output\n",
      " |      table, corresponding to the observation IDs. For example, the default\n",
      " |      will look something like:\n",
      " |      \n",
      " |          #OTU ID     Sample1 Sample2\n",
      " |          OTU1        10      2\n",
      " |          OTU2        4       8\n",
      " |  \n",
      " |  descriptive_equality(self, other)\n",
      " |      For use in testing, describe how the tables are not equal\n",
      " |  \n",
      " |  exists(self, id, axis='sample')\n",
      " |      Returns whether id exists in axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id: str\n",
      " |          id to check if exists\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to check\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          ``True`` if `id` exists, ``False`` otherwise\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Check whether sample ID is in the table:\n",
      " |      \n",
      " |      >>> table.exists('S1')\n",
      " |      True\n",
      " |      >>> table.exists('S4')\n",
      " |      False\n",
      " |      \n",
      " |      Check whether an observation ID is in the table:\n",
      " |      \n",
      " |      >>> table.exists('O1', 'observation')\n",
      " |      True\n",
      " |      >>> table.exists('O3', 'observation')\n",
      " |      False\n",
      " |  \n",
      " |  filter(self, ids_to_keep, axis='sample', invert=False, inplace=True)\n",
      " |      Filter a table based on a function or iterable.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ids_to_keep : iterable, or function(values, id, metadata) -> bool\n",
      " |          If a function, it will be called with the values of the\n",
      " |          sample/observation, its id (a string) and the dictionary\n",
      " |          of metadata of each sample/observation, and must return a\n",
      " |          boolean. If it's an iterable, it must be a list of ids to\n",
      " |          keep.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          It controls whether to filter samples or observations and\n",
      " |          defaults to \"sample\".\n",
      " |      invert : bool, optional\n",
      " |          Defaults to ``False``. If set to ``True``, discard samples or\n",
      " |          observations where `ids_to_keep` returns True\n",
      " |      inplace : bool, optional\n",
      " |          Defaults to ``True``. Whether to return a new table or modify\n",
      " |          itself.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          Returns itself if `inplace`, else returns a new filtered table.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table, with observation metadata and sample\n",
      " |      metadata:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'full_genome_available': True},\n",
      " |      ...                {'full_genome_available': False}],\n",
      " |      ...               [{'sample_type': 'a'}, {'sample_type': 'a'},\n",
      " |      ...                {'sample_type': 'b'}])\n",
      " |      \n",
      " |      Define a function to keep only samples with sample_type == 'a'. This\n",
      " |      will drop sample S3, which has sample_type 'b':\n",
      " |      \n",
      " |      >>> filter_fn = lambda val, id_, md: md['sample_type'] == 'a'\n",
      " |      \n",
      " |      Get a filtered version of the table, leaving the original table\n",
      " |      untouched:\n",
      " |      \n",
      " |      >>> new_table = table.filter(filter_fn, inplace=False)\n",
      " |      >>> print table.ids()\n",
      " |      ['S1' 'S2' 'S3']\n",
      " |      >>> print new_table.ids()\n",
      " |      ['S1' 'S2']\n",
      " |      \n",
      " |      Using the same filtering function, discard all samples with sample_type\n",
      " |      'a'. This will keep only sample S3, which has sample_type 'b':\n",
      " |      \n",
      " |      >>> new_table = table.filter(filter_fn, inplace=False, invert=True)\n",
      " |      >>> print table.ids()\n",
      " |      ['S1' 'S2' 'S3']\n",
      " |      >>> print new_table.ids()\n",
      " |      ['S3']\n",
      " |      \n",
      " |      Filter the table in-place using the same function (drop all samples\n",
      " |      where sample_type is not 'a'):\n",
      " |      \n",
      " |      >>> table.filter(filter_fn)\n",
      " |      2 x 2 <class 'biom.table.Table'> with 2 nonzero entries (50% dense)\n",
      " |      >>> print table.ids()\n",
      " |      ['S1' 'S2']\n",
      " |      \n",
      " |      Filter out all observations in the table that do not have\n",
      " |      full_genome_available == True. This will filter out observation O2:\n",
      " |      \n",
      " |      >>> filter_fn = lambda val, id_, md: md['full_genome_available']\n",
      " |      >>> table.filter(filter_fn, axis='observation')\n",
      " |      1 x 2 <class 'biom.table.Table'> with 0 nonzero entries (0% dense)\n",
      " |      >>> print table.ids(axis='observation')\n",
      " |      ['O1']\n",
      " |  \n",
      " |  get_table_density(self)\n",
      " |      Returns the fraction of nonzero elements in the table.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The fraction of nonzero elements in the table\n",
      " |  \n",
      " |  get_value_by_ids(self, obs_id, samp_id)\n",
      " |      Return value in the matrix corresponding to ``(obs_id, samp_id)``\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      obs_id : str\n",
      " |          The ID of the observation\n",
      " |      samp_id : str\n",
      " |          The ID of the sample\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          The data value corresponding to the specified matrix position\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'Z3'])\n",
      " |      \n",
      " |      Retrieve the number of counts for observation `O1` in sample `Z3`.\n",
      " |      \n",
      " |      >>> print table.get_value_by_ids('O2', 'Z3')\n",
      " |      42.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Table.data\n",
      " |  \n",
      " |  group_metadata(self, axis='sample')\n",
      " |      Return the group metadata of the given axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          Axis to search for the group metadata. Defaults to 'sample'\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          The corresponding group metadata for the given axis\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table, with group observation metadata and no group\n",
      " |      sample metadata:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> group_observation_md = {'tree': ('newick', '(O1:0.3,O2:0.4);')}\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               observation_group_metadata=group_observation_md)\n",
      " |      \n",
      " |      Get the observation group metadata:\n",
      " |      \n",
      " |      >>> table.group_metadata(axis='observation')\n",
      " |      {'tree': ('newick', '(O1:0.3,O2:0.4);')}\n",
      " |      \n",
      " |      Get the sample group metadata:\n",
      " |      \n",
      " |      >> table.group_metadata()\n",
      " |      None\n",
      " |  \n",
      " |  head(self, n=5, m=5)\n",
      " |      Get the first n rows and m columns from self\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          The number of rows (observations) to get. This number must be\n",
      " |          greater than 0. If not specified, 5 rows will be retrieved.\n",
      " |      \n",
      " |      m : int, optional\n",
      " |          The number of columns (samples) to get. This number must be\n",
      " |          greater than 0. If not specified, 5 columns will be\n",
      " |          retrieved.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Like `head` for Linux like systems, requesting more rows (or columns)\n",
      " |      than exists will silently work.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IndexError\n",
      " |          If `n` or `m` are <= 0.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          The subset table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> data = np.arange(100).reshape(5, 20)\n",
      " |      >>> obs_ids = ['O%d' % i for i in range(1, 6)]\n",
      " |      >>> samp_ids = ['S%d' % i for i in range(1, 21)]\n",
      " |      >>> table = Table(data, obs_ids, samp_ids)\n",
      " |      >>> print table.head()  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3  S4  S5\n",
      " |      O1  0.0 1.0 2.0 3.0 4.0\n",
      " |      O2  20.0 21.0 22.0 23.0 24.0\n",
      " |      O3  40.0 41.0 42.0 43.0 44.0\n",
      " |      O4  60.0 61.0 62.0 63.0 64.0\n",
      " |      O5  80.0 81.0 82.0 83.0 84.0\n",
      " |  \n",
      " |  ids(self, axis='sample')\n",
      " |      Return the ids along the given axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          Axis to return ids from. Defaults to 'sample'\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      1-D numpy array\n",
      " |          The ids along the given axis\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Get the ids along the observation axis:\n",
      " |      \n",
      " |      >>> print table.ids(axis='observation')\n",
      " |      ['O1' 'O2']\n",
      " |      \n",
      " |      Get the ids along the sample axis:\n",
      " |      \n",
      " |      >>> print table.ids()\n",
      " |      ['S1' 'S2' 'S3']\n",
      " |  \n",
      " |  index(self, id, axis)\n",
      " |      Return the index of the identified sample/observation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id : str\n",
      " |          ID of the sample or observation whose index will be returned.\n",
      " |      axis : {'sample', 'observation'}\n",
      " |          Axis to search for `id`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Index of the sample/observation identified by `id`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      UnknownIDError\n",
      " |          If provided an unrecognized sample/observation ID.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Get the index of the observation with ID \"O2\":\n",
      " |      \n",
      " |      >>> table.index('O2', 'observation')\n",
      " |      1\n",
      " |      \n",
      " |      Get the index of the sample with ID \"S1\":\n",
      " |      \n",
      " |      >>> table.index('S1', 'sample')\n",
      " |      0\n",
      " |  \n",
      " |  is_empty(self)\n",
      " |      Check whether the table is empty\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          ``True`` if the table is empty, ``False`` otherwise\n",
      " |  \n",
      " |  iter(self, dense=True, axis='sample')\n",
      " |      Yields ``(value, id, metadata)``\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dense : bool, optional\n",
      " |          Defaults to ``True``. If ``False``, yield compressed sparse row or\n",
      " |          compressed sparse columns if `axis` is 'observation' or 'sample',\n",
      " |          respectively.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to iterate over.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeneratorType\n",
      " |          A generator that yields (values, id, metadata)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'Z3'])\n",
      " |      \n",
      " |      Iter over samples and keep those that start with an Z:\n",
      " |      \n",
      " |      >>> [(values, id, metadata)\n",
      " |      ...     for values, id, metadata in table.iter() if id[0]=='Z']\n",
      " |      [(array([  1.,  42.]), 'Z3', None)]\n",
      " |      \n",
      " |      Iter over observations and add the 2nd column of the values\n",
      " |      \n",
      " |      >>> col = [values[1] for values, id, metadata in table.iter()]\n",
      " |      >>> sum(col)\n",
      " |      46.0\n",
      " |  \n",
      " |  iter_data(self, dense=True, axis='sample')\n",
      " |      Yields axis values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dense : bool, optional\n",
      " |          Defaults to ``True``. If ``False``, yield compressed sparse row or\n",
      " |          compressed sparse columns if `axis` is 'observation' or 'sample',\n",
      " |          respectively.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          Axis to iterate over.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      generator\n",
      " |          Yields list of values for each value in `axis`\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If axis other than 'sample' or 'observation' passed\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> data = np.arange(30).reshape(3,10) # 3 X 10 OTU X Sample table\n",
      " |      >>> obs_ids = ['o1', 'o2', 'o3']\n",
      " |      >>> sam_ids = ['s%i' %i for i in range(1,11)]\n",
      " |      >>> bt = Table(data, observation_ids=obs_ids, sample_ids=sam_ids)\n",
      " |      \n",
      " |      Lets find the sample with the largest sum\n",
      " |      \n",
      " |      >>> sample_gen = bt.iter_data(axis='sample')\n",
      " |      >>> max_sample_count = max([sample.sum() for sample in sample_gen])\n",
      " |      >>> print max_sample_count\n",
      " |      57.0\n",
      " |  \n",
      " |  iter_pairwise(self, dense=True, axis='sample', tri=True, diag=False)\n",
      " |      Pairwise iteration over self\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dense : bool, optional\n",
      " |          Defaults to ``True``. If ``False``, yield compressed sparse row or\n",
      " |          compressed sparse columns if `axis` is 'observation' or 'sample',\n",
      " |          respectively.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to iterate over.\n",
      " |      tri : bool, optional\n",
      " |          If ``True``, just yield [i, j] and not [j, i]\n",
      " |      diag : bool, optional\n",
      " |          If ``True``, yield [i, i]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeneratorType\n",
      " |          Yields [(val_i, id_i, metadata_i), (val_j, id_j, metadata_j)]\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      \n",
      " |      By default, only the upper triangle without the diagonal  of the\n",
      " |      resulting pairwise combinations is yielded.\n",
      " |      \n",
      " |      >>> iter_ = example_table.iter_pairwise()\n",
      " |      >>> for (val_i, id_i, md_i), (val_j, id_j, md_j) in iter_:\n",
      " |      ...     print id_i, id_j\n",
      " |      S1 S2\n",
      " |      S1 S3\n",
      " |      S2 S3\n",
      " |      \n",
      " |      The full pairwise combinations can also be yielded though.\n",
      " |      \n",
      " |      >>> iter_ = example_table.iter_pairwise(tri=False, diag=True)\n",
      " |      >>> for (val_i, id_i, md_i), (val_j, id_j, md_j) in iter_:\n",
      " |      ...     print id_i, id_j\n",
      " |      S1 S1\n",
      " |      S1 S2\n",
      " |      S1 S3\n",
      " |      S2 S1\n",
      " |      S2 S2\n",
      " |      S2 S3\n",
      " |      S3 S1\n",
      " |      S3 S2\n",
      " |      S3 S3\n",
      " |  \n",
      " |  length(self, axis='sample')\n",
      " |      Return the length of an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> print example_table.length(axis='sample')\n",
      " |      3\n",
      " |      >>> print example_table.length(axis='observation')\n",
      " |      2\n",
      " |  \n",
      " |  max(self, axis='sample')\n",
      " |      Get the maximum nonzero value over an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation', 'whole'}, optional\n",
      " |          Defaults to \"sample\". The axis over which to calculate maxima.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scalar of self.dtype or np.array of self.dtype\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> print example_table.max(axis='observation')\n",
      " |      [ 2.  5.]\n",
      " |  \n",
      " |  merge(self, other, sample='union', observation='union', sample_metadata_f=<function prefer_self>, observation_metadata_f=<function prefer_self>)\n",
      " |      Merge two tables together\n",
      " |      \n",
      " |      The axes, samples and observations, can be controlled independently.\n",
      " |      Both can work on either \"union\" or \"intersection\".\n",
      " |      \n",
      " |      `sample_metadata_f` and `observation_metadata_f` define how to\n",
      " |      merge metadata between tables. The default is to just keep the metadata\n",
      " |      associated to self if self has metadata otherwise take metadata from\n",
      " |      other. These functions are given both metadata dicts and must return\n",
      " |      a single metadata dict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : biom.Table\n",
      " |          The other table to merge with this one\n",
      " |      sample : {'union', 'intersection'}, optional\n",
      " |      observation : {'union', 'intersection'}, optional\n",
      " |      sample_metadata_f : function, optional\n",
      " |          Defaults to ``biom.util.prefer_self``. Defines how to handle sample\n",
      " |          metadata during merge.\n",
      " |      obesrvation_metadata_f : function, optional\n",
      " |          Defaults to ``biom.util.prefer_self``. Defines how to handle\n",
      " |          observation metdata during merge.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          The merged table\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      - There is an implicit type conversion to ``float``.\n",
      " |      - The return type is always that of ``self``\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x2 table and a 3x2 table:\n",
      " |      \n",
      " |      >>> d_a = np.asarray([[2, 0], [6, 1]])\n",
      " |      >>> t_a = Table(d_a, ['O1', 'O2'], ['S1', 'S2'])\n",
      " |      >>> d_b = np.asarray([[4, 5], [0, 3], [10, 10]])\n",
      " |      >>> t_b = Table(d_b, ['O1', 'O2', 'O3'], ['S1', 'S2'])\n",
      " |      \n",
      " |      Merging the table results in the overlapping samples/observations (see\n",
      " |      `O1` and `S2`) to be summed and the non-overlapping ones to be added to\n",
      " |      the resulting table (see `S3`).\n",
      " |      \n",
      " |      >>> merged_table = t_a.merge(t_b)\n",
      " |      >>> print merged_table  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1      S2\n",
      " |      O1      6.0     5.0\n",
      " |      O2      6.0     4.0\n",
      " |      O3      10.0    10.0\n",
      " |  \n",
      " |  metadata(self, id=None, axis='sample')\n",
      " |      Return the metadata of the identified sample/observation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id : str\n",
      " |          ID of the sample or observation whose index will be returned.\n",
      " |      axis : {'sample', 'observation'}\n",
      " |          Axis to search for `id`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      defaultdict or None\n",
      " |          The corresponding metadata ``defaultdict`` or ``None`` of that axis\n",
      " |          does not have metadata.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      UnknownIDError\n",
      " |          If provided an unrecognized sample/observation ID.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table, with observation metadata and no sample\n",
      " |      metadata:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'foo': 'bar'}, {'x': 'y'}], None)\n",
      " |      \n",
      " |      Get the metadata of the observation with ID \"O2\":\n",
      " |      \n",
      " |      >>> # casting to `dict` as the return is `defaultdict`\n",
      " |      >>> dict(table.metadata('O2', 'observation'))\n",
      " |      {'x': 'y'}\n",
      " |      \n",
      " |      Get the metadata of the sample with ID \"S1\":\n",
      " |      \n",
      " |      >>> table.metadata('S1', 'sample') is None\n",
      " |      True\n",
      " |  \n",
      " |  metadata_to_dataframe(self, axis)\n",
      " |      Convert axis metadata to a Pandas DataFrame\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}\n",
      " |          The axis to operate on.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pd.DataFrame\n",
      " |          A DataFrame indexed by the ids of the desired axis, columns by the\n",
      " |          metadata keys over that axis.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If the requested axis isn't recognized\n",
      " |      KeyError\n",
      " |          IF the requested axis does not have metadata\n",
      " |      TypeError\n",
      " |          If a metadata column is a list or tuple, but is jagged over the\n",
      " |          axis.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Nested metadata (e.g., KEGG_Pathways) is not supported.\n",
      " |      \n",
      " |      Metadata which are lists or tuples (e.g., taxonomy) are expanded such\n",
      " |      that each index position is a unique column. For instance, the key\n",
      " |      taxonomy will become \"taxonomy_0\", \"taxonomy_1\", etc where \"taxonomy_0\"\n",
      " |      corresponds to the 0th index position of the taxonomy.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> example_table.metadata_to_dataframe('observation')\n",
      " |         taxonomy_0     taxonomy_1\n",
      " |      O1   Bacteria     Firmicutes\n",
      " |      O2   Bacteria  Bacteroidetes\n",
      " |  \n",
      " |  min(self, axis='sample')\n",
      " |      Get the minimum nonzero value over an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation', 'whole'}, optional\n",
      " |          Defaults to \"sample\". The axis over which to calculate minima.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scalar of self.dtype or np.array of self.dtype\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> print example_table.min(axis='sample')\n",
      " |      [ 3.  1.  2.]\n",
      " |  \n",
      " |  nonzero(self)\n",
      " |      Yields locations of nonzero elements within the data matrix\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      generator\n",
      " |          Yields ``(observation_id, sample_id)`` for each nonzero element\n",
      " |  \n",
      " |  nonzero_counts(self, axis, binary=False)\n",
      " |      Get nonzero summaries about an axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation', 'whole'}\n",
      " |          The axis on which to count nonzero entries\n",
      " |      binary : bool, optional\n",
      " |          Defaults to ``False``. If ``True``, return number of nonzero\n",
      " |          entries. If ``False``, sum the values of the entries.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Counts in index order to the axis\n",
      " |  \n",
      " |  norm(self, axis='sample', inplace=True)\n",
      " |      Normalize in place sample values by an observation, or vice versa.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to use for normalization.\n",
      " |      inplace : bool, optional\n",
      " |          Defaults to ``True``. If ``True``, performs the normalization in\n",
      " |          place. Otherwise, returns a new table with the normalization\n",
      " |          applied.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          The normalized table\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x2 table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[2, 0], [6, 1]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2'])\n",
      " |      \n",
      " |      Get a version of the table normalized on the 'sample' axis, leaving the\n",
      " |      original table untouched:\n",
      " |      \n",
      " |      >>> new_table = table.norm(inplace=False)\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  2.0 0.0\n",
      " |      O2  6.0 1.0\n",
      " |      >>> print new_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  0.25    0.0\n",
      " |      O2  0.75    1.0\n",
      " |      \n",
      " |      Get a version of the table normalized on the 'observation' axis,\n",
      " |      again leaving the original table untouched:\n",
      " |      \n",
      " |      >>> new_table = table.norm(axis='observation', inplace=False)\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  2.0 0.0\n",
      " |      O2  6.0 1.0\n",
      " |      >>> print new_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  1.0 0.0\n",
      " |      O2  0.857142857143  0.142857142857\n",
      " |      \n",
      " |      Do the same normalization on 'observation', this time in-place:\n",
      " |      \n",
      " |      >>> table.norm(axis='observation')\n",
      " |      2 x 2 <class 'biom.table.Table'> with 3 nonzero entries (75% dense)\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  1.0 0.0\n",
      " |      O2  0.857142857143  0.142857142857\n",
      " |  \n",
      " |  pa(self, inplace=True)\n",
      " |      Convert the table to presence/absence data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inplace : bool, optional\n",
      " |          Defaults to ``False``\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          Returns itself if `inplace`, else returns a new presence/absence\n",
      " |          table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> import numpy as np\n",
      " |      \n",
      " |      Create a 2x3 BIOM table\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Convert to presence/absence data\n",
      " |      \n",
      " |      >>> _ = table.pa()\n",
      " |      >>> print table.data('O1', 'observation')\n",
      " |      [ 0.  0.  1.]\n",
      " |      >>> print table.data('O2', 'observation')\n",
      " |      [ 1.  1.  1.]\n",
      " |  \n",
      " |  partition(self, f, axis='sample')\n",
      " |      Yields partitions\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          `f` is given the ID and metadata of the vector and must return\n",
      " |          what partition the vector is part of.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to iterate over\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      GeneratorType\n",
      " |          A generator that yields (partition, `Table`)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> from biom.util import unzip\n",
      " |      \n",
      " |      Create a 2x3 BIOM table, with observation metadata and sample\n",
      " |      metadata:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'full_genome_available': True},\n",
      " |      ...                {'full_genome_available': False}],\n",
      " |      ...               [{'sample_type': 'a'}, {'sample_type': 'a'},\n",
      " |      ...                {'sample_type': 'b'}])\n",
      " |      \n",
      " |      Define a function to bin by sample_type\n",
      " |      \n",
      " |      >>> f = lambda id_, md: md['sample_type']\n",
      " |      \n",
      " |      Partition the table and view results\n",
      " |      \n",
      " |      >>> bins, tables = table.partition(f)\n",
      " |      >>> print bins[1] # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2\n",
      " |      O1  0.0 0.0\n",
      " |      O2  1.0 3.0\n",
      " |      >>> print tables[1] # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S3\n",
      " |      O1  1.0\n",
      " |      O2  42.0\n",
      " |  \n",
      " |  rankdata(self, axis='sample', inplace=True, method='average')\n",
      " |      Convert values to rank abundances from smallest to largest\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to use for ranking.\n",
      " |      inplace : bool, optional\n",
      " |          Defaults to ``True``. If ``True``, performs the ranking in\n",
      " |          place. Otherwise, returns a new table with ranking applied.\n",
      " |      method : str, optional\n",
      " |          The method for handling ties in counts. This can be any valid\n",
      " |          string that can be passed to `scipy.stats.rankdata`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          The rank-abundance-transformed table.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If unknown ``method`` is provided.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      scipy.stats.rankdata\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom import Table\n",
      " |      >>> data = np.array([[ 99,  12,   8], [  0,  42,   7],\n",
      " |      ...                  [112,  42,   6], [  5,  75,   5]])\n",
      " |      >>> t = Table(data, sample_ids=['s1', 's2', 's3'],\n",
      " |      ...           observation_ids=['o1', 'o2', 'o3', 'o4'])\n",
      " |      \n",
      " |      Convert observation counts to their ranked abundance, from smallest\n",
      " |      to largest.\n",
      " |      \n",
      " |      >>> print t.rankdata()  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID s1      s2      s3\n",
      " |      o1      2.0     1.0     4.0\n",
      " |      o2      0.0     2.5     3.0\n",
      " |      o3      3.0     2.5     2.0\n",
      " |      o4      1.0     4.0     1.0\n",
      " |  \n",
      " |  reduce(self, f, axis)\n",
      " |      Reduce over axis using function `f`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          The function to use for the reduce operation\n",
      " |      axis : {'sample', 'observation'}\n",
      " |          The axis on which to operate\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          A one-dimensional array representing the reduced rows\n",
      " |          (observations) or columns (samples) of the data matrix\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If `axis` is neither \"sample\" nor \"observation\"\n",
      " |      TableException\n",
      " |          If the table's data matrix is empty\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 table\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'foo': 'bar'}, {'x': 'y'}], None)\n",
      " |      \n",
      " |      Create a reduce function\n",
      " |      \n",
      " |      >>> func = lambda x, y: x + y\n",
      " |      \n",
      " |      Reduce table on samples\n",
      " |      \n",
      " |      >>> table.reduce(func, 'sample') # doctest: +NORMALIZE_WHITESPACE\n",
      " |      array([  1.,   3.,  43.])\n",
      " |      \n",
      " |      Reduce table on observations\n",
      " |      \n",
      " |      >>> table.reduce(func, 'observation') # doctest: +NORMALIZE_WHITESPACE\n",
      " |      array([  1.,  46.])\n",
      " |  \n",
      " |  remove_empty(self, axis='whole', inplace=True)\n",
      " |      Remove empty samples or observations from the table\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'whole', 'sample', 'observation'}, optional\n",
      " |          The axis on which to operate.\n",
      " |      inplace : bool, optional\n",
      " |          If ``True`` vectors are removed in ``self``; if ``False`` the\n",
      " |          vectors are removed in a new table is returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If the axis is not recognized.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          A table object with the zero'd rows, or columns removed as\n",
      " |          specified by the `axis` parameter.\n",
      " |  \n",
      " |  sort(self, sort_f=<function natsort>, axis='sample')\n",
      " |      Return a table sorted along axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sort_f : function, optional\n",
      " |          Defaults to ``biom.util.natsort``. A function that takes a list of\n",
      " |          values and sorts it\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          A table whose samples or observations are sorted according to the\n",
      " |          `sort_f` function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[1, 0, 4], [1, 3, 0]])\n",
      " |      >>> table = Table(data, ['O2', 'O1'], ['S2', 'S1', 'S3'])\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S2  S1  S3\n",
      " |      O2  1.0 0.0 4.0\n",
      " |      O1  1.0 3.0 0.0\n",
      " |      \n",
      " |      Sort the order of samples in the table using the default natural\n",
      " |      sorting:\n",
      " |      \n",
      " |      >>> new_table = table.sort()\n",
      " |      >>> print new_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O2  0.0 1.0 4.0\n",
      " |      O1  3.0 1.0 0.0\n",
      " |      \n",
      " |      Sort the order of observations in the table using the default natural\n",
      " |      sorting:\n",
      " |      \n",
      " |      >>> new_table = table.sort(axis='observation')\n",
      " |      >>> print new_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S2  S1  S3\n",
      " |      O1  1.0 3.0 0.0\n",
      " |      O2  1.0 0.0 4.0\n",
      " |      \n",
      " |      Sort the samples in reverse order using a custom sort function:\n",
      " |      \n",
      " |      >>> sort_f = lambda x: list(sorted(x, reverse=True))\n",
      " |      >>> new_table = table.sort(sort_f=sort_f)\n",
      " |      >>> print new_table  # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S3  S2  S1\n",
      " |      O2  4.0 1.0 0.0\n",
      " |      O1  0.0 1.0 3.0\n",
      " |  \n",
      " |  sort_order(self, order, axis='sample')\n",
      " |      Return a new table with `axis` in `order`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : iterable\n",
      " |          The desired order for axis\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          A table where the observations or samples are sorted according to\n",
      " |          `order`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[1, 0, 4], [1, 3, 0]])\n",
      " |      >>> table = Table(data, ['O2', 'O1'], ['S2', 'S1', 'S3'])\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S2  S1  S3\n",
      " |      O2  1.0 0.0 4.0\n",
      " |      O1  1.0 3.0 0.0\n",
      " |      \n",
      " |      Sort the table using a list of samples:\n",
      " |      \n",
      " |      >>> sorted_table = table.sort_order(['S2', 'S3', 'S1'])\n",
      " |      >>> print sorted_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S2      S3      S1\n",
      " |      O2      1.0     4.0     0.0\n",
      " |      O1      1.0     0.0     3.0\n",
      " |      \n",
      " |      \n",
      " |      Additionally you could sort the table's observations:\n",
      " |      \n",
      " |      >>> sorted_table = table.sort_order(['O1', 'O2'], axis=\"observation\")\n",
      " |      >>> print sorted_table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S2      S1      S3\n",
      " |      O1      1.0     3.0     0.0\n",
      " |      O2      1.0     0.0     4.0\n",
      " |  \n",
      " |  subsample(self, n, axis='sample', by_id=False)\n",
      " |      Randomly subsample without replacement.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int\n",
      " |          Number of items to subsample from `counts`.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to sample over\n",
      " |      by_id : boolean, optional\n",
      " |          If `False`, the subsampling is based on the counts contained in the\n",
      " |          matrix (e.g., rarefaction). If `True`, the subsampling is based on\n",
      " |          the IDs (e.g., fetch a random subset of samples). Default is\n",
      " |          `False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          A subsampled version of self\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If `n` is less than zero.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Subsampling is performed without replacement. If `n` is greater than\n",
      " |      the sum of a given vector, that vector is omitted from the result.\n",
      " |      \n",
      " |      Adapted from `skbio.math.subsample`, see biom-format/licenses for more\n",
      " |      information about scikit-bio.\n",
      " |      \n",
      " |      This code assumes absolute abundance if `by_id` is False.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> table = Table(np.array([[0, 2, 3], [1, 0, 2]]), ['O1', 'O2'],\n",
      " |      ...               ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Subsample 1 item over the sample axis by value (e.g., rarefaction):\n",
      " |      \n",
      " |      >>> print table.subsample(1).sum(axis='sample')\n",
      " |      [ 1.  1.  1.]\n",
      " |      \n",
      " |      Subsample 2 items over the sample axis, note that 'S1' is filtered out:\n",
      " |      \n",
      " |      >>> ss = table.subsample(2)\n",
      " |      >>> print ss.sum(axis='sample')\n",
      " |      [ 2.  2.]\n",
      " |      >>> print ss.ids()\n",
      " |      ['S2' 'S3']\n",
      " |      \n",
      " |      Subsample by IDs over the sample axis. For this example, we're going to\n",
      " |      randomly select 2 samples and do this 100 times, and then print out the\n",
      " |      set of IDs observed.\n",
      " |      \n",
      " |      >>> ids = set([tuple(table.subsample(2, by_id=True).ids())\n",
      " |      ...            for i in range(100)])\n",
      " |      >>> print sorted(ids)\n",
      " |      [('S1', 'S2'), ('S1', 'S3'), ('S2', 'S3')]\n",
      " |  \n",
      " |  sum(self, axis='whole')\n",
      " |      Returns the sum by axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'whole', 'sample', 'observation'}, optional\n",
      " |          The axis on which to operate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array or float\n",
      " |          If `axis` is \"whole\", returns an float representing the whole\n",
      " |          table sum. If `axis` is either \"sample\" or \"observation\", returns a\n",
      " |          numpy.array that holds a sum for each sample or observation,\n",
      " |          respectively.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Add all values in the table:\n",
      " |      \n",
      " |      >>> table.sum()\n",
      " |      array(47.0)\n",
      " |      \n",
      " |      Add all values per sample:\n",
      " |      \n",
      " |      >>> table.sum(axis='sample') # doctest: +NORMALIZE_WHITESPACE\n",
      " |      array([  1.,  3.,  43.])\n",
      " |      \n",
      " |      Add all values per observation:\n",
      " |      \n",
      " |      >>> table.sum(axis='observation') # doctest: +NORMALIZE_WHITESPACE\n",
      " |      array([  1.,  46.])\n",
      " |  \n",
      " |  to_dataframe(self)\n",
      " |      Convert matrix data to a Pandas SparseDataFrame\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pd.SparseDataFrame\n",
      " |          A SparseDataFrame indexed on the observation IDs, with the column\n",
      " |          names as the sample IDs.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Metadata are not included.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import example_table\n",
      " |      >>> df = example_table.to_dataframe()\n",
      " |      >>> df\n",
      " |           S1   S2   S3\n",
      " |      O1  0.0  1.0  2.0\n",
      " |      O2  3.0  4.0  5.0\n",
      " |  \n",
      " |  to_hdf5(self, h5grp, generated_by, compress=True, format_fs=None)\n",
      " |      Store CSC and CSR in place\n",
      " |      \n",
      " |      The resulting structure of this group is below. A few basic\n",
      " |      definitions, N is the number of observations and M is the number of\n",
      " |      samples. Data are stored in both compressed sparse row [1]_ (CSR, for\n",
      " |      observation oriented operations) and compressed sparse column [2]_\n",
      " |      (CSC, for sample oriented operations).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The expected HDF5 group structure is below. An example of an HDF5 file\n",
      " |      in DDL can be found here [3]_.\n",
      " |      \n",
      " |      - ./id                                                  : str, an arbitrary ID\n",
      " |      - ./type                                                : str, the table type (e.g, OTU table)\n",
      " |      - ./format-url                                          : str, a URL that describes the format\n",
      " |      - ./format-version                                      : two element tuple of int32, major and minor\n",
      " |      - ./generated-by                                        : str, what generated this file\n",
      " |      - ./creation-date                                       : str, ISO format\n",
      " |      - ./shape                                               : two element tuple of int32, N by M\n",
      " |      - ./nnz                                                 : int32 or int64, number of non zero elems\n",
      " |      - ./observation                                         : Group\n",
      " |      - ./observation/ids                                     : (N,) dataset of str or vlen str\n",
      " |      - ./observation/matrix                                  : Group\n",
      " |      - ./observation/matrix/data                             : (nnz,) dataset of float64\n",
      " |      - ./observation/matrix/indices                          : (nnz,) dataset of int32\n",
      " |      - ./observation/matrix/indptr                           : (M+1,) dataset of int32\n",
      " |      - ./observation/metadata                                : Group\n",
      " |      - [./observation/metadata/foo]                          : Optional, (N,) dataset of any valid HDF5 type in index order with IDs.\n",
      " |      - ./observation/group-metadata                          : Group\n",
      " |      - [./observation/group-metadata/foo]                    : Optional, (?,) dataset of group metadata that relates IDs\n",
      " |      - [./observation/group-metadata/foo.attrs['data_type']] : attribute of the foo dataset that describes contained type (e.g., newick)\n",
      " |      - ./sample                                              : Group\n",
      " |      - ./sample/ids                                          : (M,) dataset of str or vlen str\n",
      " |      - ./sample/matrix                                       : Group\n",
      " |      - ./sample/matrix/data                                  : (nnz,) dataset of float64\n",
      " |      - ./sample/matrix/indices                               : (nnz,) dataset of int32\n",
      " |      - ./sample/matrix/indptr                                : (N+1,) dataset of int32\n",
      " |      - ./sample/metadata                                     : Group\n",
      " |      - [./sample/metadata/foo]                               : Optional, (M,) dataset of any valid HDF5 type in index order with IDs.\n",
      " |      - ./sample/group-metadata                               : Group\n",
      " |      - [./sample/group-metadata/foo]                         : Optional, (?,) dataset of group metadata that relates IDs\n",
      " |      - [./sample/group-metadata/foo.attrs['data_type']]      : attribute of the foo dataset that describes contained type (e.g., newick)\n",
      " |      \n",
      " |      The '?' character on the dataset size means that it can be of arbitrary\n",
      " |      length.\n",
      " |      \n",
      " |      The expected structure for each of the metadata datasets is a list of\n",
      " |      atomic type objects (int, float, str, ...), where the index order of\n",
      " |      the list corresponds to the index order of the relevant axis IDs.\n",
      " |      Special metadata fields have been defined, and they are stored in a\n",
      " |      specific way. Currently, the available special metadata fields are:\n",
      " |      \n",
      " |      - taxonomy: (N, ?) dataset of str or vlen str\n",
      " |      - KEGG_Pathways: (N, ?) dataset of str or vlen str\n",
      " |      - collapsed_ids: (N, ?) dataset of str or vlen str\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      h5grp : `h5py.Group` or `h5py.File`\n",
      " |          The HDF5 entity in which to write the BIOM formatted data.\n",
      " |      generated_by : str\n",
      " |          A description of what generated the table\n",
      " |      compress : bool, optional\n",
      " |          Defaults to ``True`` means fields will be compressed with gzip,\n",
      " |          ``False`` means no compression\n",
      " |      format_fs : dict, optional\n",
      " |          Specify custom formatting functions for metadata fields. This dict\n",
      " |          is expected to be {'metadata_field': function}, where the function\n",
      " |          signature is (h5py.Group, str, dict, bool) corresponding to the\n",
      " |          specific HDF5 group the metadata dataset will be associated with,\n",
      " |          the category being operated on, the metadata for the entire axis\n",
      " |          being operated on, and whether to enable compression on the\n",
      " |          dataset.  Anything returned by this function is ignored.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method does not return anything and operates in place on h5grp.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Table.from_hdf5\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.sparse.csr_matrix.html\n",
      " |      .. [2] http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.sparse.csc_matrix.html\n",
      " |      .. [3] http://biom-format.org/documentation/format_versions/biom-2.1.html\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom.util import biom_open  # doctest: +SKIP\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> from numpy import array\n",
      " |      >>> t = Table(array([[1, 2], [3, 4]]), ['a', 'b'], ['x', 'y'])\n",
      " |      >>> with biom_open('foo.biom', 'w') as f:  # doctest: +SKIP\n",
      " |      ...     t.to_hdf5(f, \"example\")\n",
      " |  \n",
      " |  to_json(self, generated_by, direct_io=None)\n",
      " |      Returns a JSON string representing the table in BIOM format.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      generated_by : str\n",
      " |          a string describing the software used to build the table\n",
      " |      direct_io : file or file-like object, optional\n",
      " |          Defaults to ``None``. Must implementing a ``write`` function. If\n",
      " |          `direct_io` is not ``None``, the final output is written directly\n",
      " |          to `direct_io` during processing.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          A JSON-formatted string representing the biom table\n",
      " |  \n",
      " |  to_tsv(self, header_key=None, header_value=None, metadata_formatter=<type 'str'>, observation_column_name='#OTU ID')\n",
      " |      Return self as a string in tab delimited form\n",
      " |      \n",
      " |      Default ``str`` output for the ``Table`` is just row/col ids and table\n",
      " |      data without any metadata\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      header_key : str or ``None``, optional\n",
      " |          Defaults to ``None``\n",
      " |      header_value : str or ``None``, optional\n",
      " |          Defaults to ``None``\n",
      " |      metadata_formatter : function, optional\n",
      " |          Defaults to ``str``.  a function which takes a metadata entry and\n",
      " |          returns a formatted version that should be written to file\n",
      " |      observation_column_name : str, optional\n",
      " |          Defaults to \"#OTU ID\". The name of the first column in the output\n",
      " |          table, corresponding to the observation IDs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          tab delimited representation of the Table\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 BIOM table, with observation metadata and no sample\n",
      " |      metadata:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'foo': 'bar'}, {'x': 'y'}], None)\n",
      " |      >>> print table.to_tsv() # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1      S2      S3\n",
      " |      O1      0.0     0.0     1.0\n",
      " |      O2      1.0     3.0     42.0\n",
      " |  \n",
      " |  transform(self, f, axis='sample', inplace=True)\n",
      " |      Iterate over `axis`, applying a function `f` to each vector.\n",
      " |      \n",
      " |      Only non null values can be modified and the density of the\n",
      " |      table can't increase. However, zeroing values is fine.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function(data, id, metadata) -> new data\n",
      " |          A function that takes three values: an array of nonzero\n",
      " |          values corresponding to each observation or sample, an\n",
      " |          observation or sample id, and an observation or sample\n",
      " |          metadata entry. It must return an array of transformed\n",
      " |          values that replace the original values.\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to operate on. Can be \"sample\" or \"observation\".\n",
      " |      inplace : bool, optional\n",
      " |          Defaults to ``True``. Whether to return a new table or modify\n",
      " |          itself.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          Returns itself if `inplace`, else returns a new transformed table.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import numpy as np\n",
      " |      >>> from biom.table import Table\n",
      " |      \n",
      " |      Create a 2x3 table\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'],\n",
      " |      ...               [{'foo': 'bar'}, {'x': 'y'}], None)\n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O1  0.0 0.0 1.0\n",
      " |      O2  1.0 3.0 42.0\n",
      " |      \n",
      " |      Create a transform function\n",
      " |      \n",
      " |      >>> f = lambda data, id_, md: data / 2\n",
      " |      \n",
      " |      Transform to a new table on samples\n",
      " |      \n",
      " |      >>> table2 = table.transform(f, 'sample', False)\n",
      " |      >>> print table2 # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O1  0.0 0.0 0.5\n",
      " |      O2  0.5 1.5 21.0\n",
      " |      \n",
      " |      `table` hasn't changed\n",
      " |      \n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O1  0.0 0.0 1.0\n",
      " |      O2  1.0 3.0 42.0\n",
      " |      \n",
      " |      Tranform in place on observations\n",
      " |      \n",
      " |      >>> table3 = table.transform(f, 'observation', True)\n",
      " |      \n",
      " |      `table` is different now\n",
      " |      \n",
      " |      >>> print table # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O1  0.0 0.0 0.5\n",
      " |      O2  0.5 1.5 21.0\n",
      " |      \n",
      " |      but the table returned (`table3`) is the same as `table`\n",
      " |      \n",
      " |      >>> print table3 # doctest: +NORMALIZE_WHITESPACE\n",
      " |      # Constructed from biom file\n",
      " |      #OTU ID S1  S2  S3\n",
      " |      O1  0.0 0.0 0.5\n",
      " |      O2  0.5 1.5 21.0\n",
      " |  \n",
      " |  transpose(self)\n",
      " |      Transpose the contingency table\n",
      " |      \n",
      " |      The returned table will be an entirely new table, including copies of\n",
      " |      the (transposed) data, sample/observation IDs and metadata.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          Return a new table that is the transpose of caller table.\n",
      " |  \n",
      " |  update_ids(self, id_map, axis='sample', strict=True, inplace=True)\n",
      " |      Update the ids along the given axis\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      id_map : dict\n",
      " |          Mapping of old to new ids\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          Axis to search for `id`. Defaults to 'sample'\n",
      " |      strict : bool, optional\n",
      " |          If ``True``, raise an error if an id is present in the given axis\n",
      " |          but is not a key in ``id_map``. If False, retain old identifier\n",
      " |          for ids that are present in the given axis but are not keys in\n",
      " |          ``id_map``.\n",
      " |      inplace : bool, optional\n",
      " |          If ``True`` the ids are updated in ``self``; if ``False`` the ids\n",
      " |          are updated in a new table is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |          Table object where ids have been updated.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      UnknownAxisError\n",
      " |          If provided an unrecognized axis.\n",
      " |      TableException\n",
      " |          If an id from ``self`` is not in ``id_map`` and ``strict`` is\n",
      " |          ``True``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a 2x3 BIOM table:\n",
      " |      \n",
      " |      >>> data = np.asarray([[0, 0, 1], [1, 3, 42]])\n",
      " |      >>> table = Table(data, ['O1', 'O2'], ['S1', 'S2', 'S3'])\n",
      " |      \n",
      " |      Define a mapping of old to new sample ids:\n",
      " |      \n",
      " |      >>> id_map = {'S1':'s1.1', 'S2':'s2.2', 'S3':'s3.3'}\n",
      " |      \n",
      " |      Get the ids along the sample axis in the table:\n",
      " |      \n",
      " |      >>> print table.ids(axis='sample')\n",
      " |      ['S1' 'S2' 'S3']\n",
      " |      \n",
      " |      Update the sample ids and get the ids along the sample axis in the\n",
      " |      updated table:\n",
      " |      \n",
      " |      >>> updated_table = table.update_ids(id_map, axis='sample')\n",
      " |      >>> print updated_table.ids(axis='sample')\n",
      " |      ['s1.1' 's2.2' 's3.3']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_hdf5(cls, h5grp, ids=None, axis='sample', parse_fs=None, subset_with_metadata=True) from __builtin__.type\n",
      " |      Parse an HDF5 formatted BIOM table\n",
      " |      \n",
      " |      If ids is provided, only the samples/observations listed in ids\n",
      " |      (depending on the value of axis) will be loaded\n",
      " |      \n",
      " |      The expected structure of this group is below. A few basic definitions,\n",
      " |      N is the number of observations and M is the number of samples. Data\n",
      " |      are stored in both compressed sparse row (for observation oriented\n",
      " |      operations) and compressed sparse column (for sample oriented\n",
      " |      operations).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The expected HDF5 group structure is below. An example of an HDF5 file\n",
      " |      in DDL can be found here [3]_.\n",
      " |      \n",
      " |      - ./id                                                  : str, an arbitrary ID\n",
      " |      - ./type                                                : str, the table type (e.g, OTU table)\n",
      " |      - ./format-url                                          : str, a URL that describes the format\n",
      " |      - ./format-version                                      : two element tuple of int32, major and minor\n",
      " |      - ./generated-by                                        : str, what generated this file\n",
      " |      - ./creation-date                                       : str, ISO format\n",
      " |      - ./shape                                               : two element tuple of int32, N by M\n",
      " |      - ./nnz                                                 : int32 or int64, number of non zero elems\n",
      " |      - ./observation                                         : Group\n",
      " |      - ./observation/ids                                     : (N,) dataset of str or vlen str\n",
      " |      - ./observation/matrix                                  : Group\n",
      " |      - ./observation/matrix/data                             : (nnz,) dataset of float64\n",
      " |      - ./observation/matrix/indices                          : (nnz,) dataset of int32\n",
      " |      - ./observation/matrix/indptr                           : (M+1,) dataset of int32\n",
      " |      - ./observation/metadata                                : Group\n",
      " |      - [./observation/metadata/foo]                          : Optional, (N,) dataset of any valid HDF5 type in index order with IDs.\n",
      " |      - ./observation/group-metadata                          : Group\n",
      " |      - [./observation/group-metadata/foo]                    : Optional, (?,) dataset of group metadata that relates IDs\n",
      " |      - [./observation/group-metadata/foo.attrs['data_type']] : attribute of the foo dataset that describes contained type (e.g., newick)\n",
      " |      - ./sample                                              : Group\n",
      " |      - ./sample/ids                                          : (M,) dataset of str or vlen str\n",
      " |      - ./sample/matrix                                       : Group\n",
      " |      - ./sample/matrix/data                                  : (nnz,) dataset of float64\n",
      " |      - ./sample/matrix/indices                               : (nnz,) dataset of int32\n",
      " |      - ./sample/matrix/indptr                                : (N+1,) dataset of int32\n",
      " |      - ./sample/metadata                                     : Group\n",
      " |      - [./sample/metadata/foo]                               : Optional, (M,) dataset of any valid HDF5 type in index order with IDs.\n",
      " |      - ./sample/group-metadata                               : Group\n",
      " |      - [./sample/group-metadata/foo]                         : Optional, (?,) dataset of group metadata that relates IDs\n",
      " |      - [./sample/group-metadata/foo.attrs['data_type']]      : attribute of the foo dataset that describes contained type (e.g., newick)\n",
      " |      \n",
      " |      The '?' character on the dataset size means that it can be of arbitrary\n",
      " |      length.\n",
      " |      \n",
      " |      The expected structure for each of the metadata datasets is a list of\n",
      " |      atomic type objects (int, float, str, ...), where the index order of\n",
      " |      the list corresponds to the index order of the relevant axis IDs.\n",
      " |      Special metadata fields have been defined, and they are stored in a\n",
      " |      specific way. Currently, the available special metadata fields are:\n",
      " |      \n",
      " |      - taxonomy: (N, ?) dataset of str or vlen str\n",
      " |      - KEGG_Pathways: (N, ?) dataset of str or vlen str\n",
      " |      - collapsed_ids: (N, ?) dataset of str or vlen str\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      h5grp : a h5py ``Group`` or an open h5py ``File``\n",
      " |      ids : iterable\n",
      " |          The sample/observation ids of the samples/observations that we need\n",
      " |          to retrieve from the hdf5 biom table\n",
      " |      axis : {'sample', 'observation'}, optional\n",
      " |          The axis to subset on\n",
      " |      parse_fs : dict, optional\n",
      " |          Specify custom parsing functions for metadata fields. This dict is\n",
      " |          expected to be {'metadata_field': function}, where the function\n",
      " |          signature is (object) corresponding to a single row in the\n",
      " |          associated metadata dataset. The return from this function an\n",
      " |          object as well, and is the parsed representation of the metadata.\n",
      " |      subset_with_metadata : bool, optional\n",
      " |          When subsetting (i.e., `ids` is `not None`), whether to also parse\n",
      " |          the metadata. By default, the metadata are also subset. The reason\n",
      " |          for exposing this functionality is that, for large tables, there\n",
      " |          exists a very large overhead for this metadata manipulation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          A BIOM ``Table`` object\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If `ids` are not a subset of the samples or observations ids\n",
      " |          present in the hdf5 biom table\n",
      " |      \n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.sparse.csr_matrix.html\n",
      " |      .. [2] http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.sparse.csc_matrix.html\n",
      " |      .. [3] http://biom-format.org/documentation/format_versions/biom-2.0.html\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Table.to_hdf5\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom.table import Table\n",
      " |      >>> from biom.util import biom_open\n",
      " |      >>> with biom_open('rich_sparse_otu_table_hdf5.biom') as f # doctest: +SKIP\n",
      " |      >>>     t = Table.from_hdf5(f) # doctest: +SKIP\n",
      " |      \n",
      " |      Parse a hdf5 biom table subsetting observations\n",
      " |      >>> from biom.util import biom_open # doctest: +SKIP\n",
      " |      >>> from biom.parse import parse_biom_table\n",
      " |      >>> with biom_open('rich_sparse_otu_table_hdf5.biom') as f # doctest: +SKIP\n",
      " |      >>>     t = Table.from_hdf5(f, ids=[\"GG_OTU_1\"],\n",
      " |      ...                         axis='observation') # doctest: +SKIP\n",
      " |  \n",
      " |  from_json(self, json_table, data_pump=None, input_is_dense=False) from __builtin__.type\n",
      " |      Parse a biom otu table type\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      json_table : dict\n",
      " |          A JSON object or dict that represents the BIOM table\n",
      " |      data_pump : tuple or None\n",
      " |          A secondary source of data\n",
      " |      input_is_dense : bool\n",
      " |          If `True`, the data contained will be interpretted as dense\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Table\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from biom import Table\n",
      " |      >>> json_obj = {\"id\": \"None\",\n",
      " |      ...             \"format\": \"Biological Observation Matrix 1.0.0\",\n",
      " |      ...             \"format_url\": \"http://biom-format.org\",\n",
      " |      ...             \"generated_by\": \"foo\",\n",
      " |      ...             \"type\": \"OTU table\",\n",
      " |      ...             \"date\": \"2014-06-03T14:24:40.884420\",\n",
      " |      ...             \"matrix_element_type\": \"float\",\n",
      " |      ...             \"shape\": [5, 6],\n",
      " |      ...             \"data\": [[0,2,1.0],\n",
      " |      ...                      [1,0,5.0],\n",
      " |      ...                      [1,1,1.0],\n",
      " |      ...                      [1,3,2.0],\n",
      " |      ...                      [1,4,3.0],\n",
      " |      ...                      [1,5,1.0],\n",
      " |      ...                      [2,2,1.0],\n",
      " |      ...                      [2,3,4.0],\n",
      " |      ...                      [2,5,2.0],\n",
      " |      ...                      [3,0,2.0],\n",
      " |      ...                      [3,1,1.0],\n",
      " |      ...                      [3,2,1.0],\n",
      " |      ...                      [3,5,1.0],\n",
      " |      ...                      [4,1,1.0],\n",
      " |      ...                      [4,2,1.0]],\n",
      " |      ...             \"rows\": [{\"id\": \"GG_OTU_1\", \"metadata\": None},\n",
      " |      ...                      {\"id\": \"GG_OTU_2\", \"metadata\": None},\n",
      " |      ...                      {\"id\": \"GG_OTU_3\", \"metadata\": None},\n",
      " |      ...                      {\"id\": \"GG_OTU_4\", \"metadata\": None},\n",
      " |      ...                      {\"id\": \"GG_OTU_5\", \"metadata\": None}],\n",
      " |      ...             \"columns\": [{\"id\": \"Sample1\", \"metadata\": None},\n",
      " |      ...                         {\"id\": \"Sample2\", \"metadata\": None},\n",
      " |      ...                         {\"id\": \"Sample3\", \"metadata\": None},\n",
      " |      ...                         {\"id\": \"Sample4\", \"metadata\": None},\n",
      " |      ...                         {\"id\": \"Sample5\", \"metadata\": None},\n",
      " |      ...                         {\"id\": \"Sample6\", \"metadata\": None}]\n",
      " |      ...             }\n",
      " |      >>> t = Table.from_json(json_obj)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_tsv(lines, obs_mapping, sample_mapping, process_func, **kwargs)\n",
      " |      Parse a tab separated (observation x sample) formatted BIOM table\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lines : list, or file-like object\n",
      " |          The tab delimited data to parse\n",
      " |      obs_mapping : dict or None\n",
      " |          The corresponding observation metadata\n",
      " |      sample_mapping : dict or None\n",
      " |          The corresponding sample metadata\n",
      " |      process_func : function\n",
      " |          A function to transform the observation metadata\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      biom.Table\n",
      " |          A BIOM ``Table`` object\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Parse tab separated data into a table:\n",
      " |      \n",
      " |      >>> from biom.table import Table\n",
      " |      >>> from StringIO import StringIO\n",
      " |      >>> tsv = 'a\\tb\\tc\\n1\\t2\\t3\\n4\\t5\\t6'\n",
      " |      >>> tsv_fh = StringIO(tsv)\n",
      " |      >>> func = lambda x : x\n",
      " |      >>> test_table = Table.from_tsv(tsv_fh, None, None, func)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dtype\n",
      " |      The type of the objects in the underlying contingency matrix\n",
      " |  \n",
      " |  matrix_data\n",
      " |      The sparse matrix object\n",
      " |  \n",
      " |  nnz\n",
      " |      Number of non-zero elements of the underlying contingency matrix\n",
      " |  \n",
      " |  shape\n",
      " |      The shape of the underlying contingency matrix\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otus = [otu for otu in table.ids(axis='observation')]\n",
    "samples = [samp for samp in table.ids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'242.O', u'919.O', u'330.O', u'445.O', u'733.O', u'931.O', u'970.O', u'439.O', u'724.O', u'991.O']\n"
     ]
    }
   ],
   "source": [
    "print samples[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'242.O' u'919.O' u'330.O' u'445.O' u'733.O' u'931.O' u'970.O' u'439.O'\n",
      " u'724.O' u'991.O']\n"
     ]
    }
   ],
   "source": [
    "filt_table = table.filter(samples[0:10],axis='sample',invert=False,inplace=False)\n",
    "print filt_table.ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>242.O</th>\n",
       "      <th>919.O</th>\n",
       "      <th>330.O</th>\n",
       "      <th>445.O</th>\n",
       "      <th>733.O</th>\n",
       "      <th>931.O</th>\n",
       "      <th>970.O</th>\n",
       "      <th>439.O</th>\n",
       "      <th>724.O</th>\n",
       "      <th>991.O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OTU_360</th>\n",
       "      <td>30963.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>53536.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13795.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_47557</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_45150</th>\n",
       "      <td>69.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_3</th>\n",
       "      <td>365.0</td>\n",
       "      <td>9387.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>29932.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>6685.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             242.O    919.O  330.O   445.O    733.O  931.O    970.O  439.O  \\\n",
       "OTU_360    30963.0      2.0    1.0    19.0      0.0    0.0      0.0    0.0   \n",
       "OTU_5          2.0  53536.0    6.0    36.0    759.0   12.0  13795.0   77.0   \n",
       "OTU_47557      0.0      0.0    1.0     0.0      0.0    0.0      0.0    0.0   \n",
       "OTU_45150     69.0     23.0  202.0  1963.0      9.0    6.0      0.0   63.0   \n",
       "OTU_3        365.0   9387.0  671.0    95.0  29932.0    6.0    183.0  485.0   \n",
       "\n",
       "           724.O   991.O  \n",
       "OTU_360      0.0     1.0  \n",
       "OTU_5       25.0   489.0  \n",
       "OTU_47557    0.0     0.0  \n",
       "OTU_45150    0.0     1.0  \n",
       "OTU_3      521.0  6685.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filt_table.to_dataframe() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>242.O</th>\n",
       "      <th>919.O</th>\n",
       "      <th>330.O</th>\n",
       "      <th>445.O</th>\n",
       "      <th>733.O</th>\n",
       "      <th>931.O</th>\n",
       "      <th>970.O</th>\n",
       "      <th>439.O</th>\n",
       "      <th>724.O</th>\n",
       "      <th>991.O</th>\n",
       "      <th>...</th>\n",
       "      <th>1319.O</th>\n",
       "      <th>1220.O</th>\n",
       "      <th>1436.O</th>\n",
       "      <th>1592.O</th>\n",
       "      <th>NTC8b</th>\n",
       "      <th>1328.O</th>\n",
       "      <th>1413.O</th>\n",
       "      <th>1459.O</th>\n",
       "      <th>1454.O</th>\n",
       "      <th>1267.O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OTU_360</th>\n",
       "      <td>30963.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>53536.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13795.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_47557</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_45150</th>\n",
       "      <td>69.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>1963.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_3</th>\n",
       "      <td>365.0</td>\n",
       "      <td>9387.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>29932.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>6685.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             242.O    919.O  330.O   445.O    733.O  931.O    970.O  439.O  \\\n",
       "OTU_360    30963.0      2.0    1.0    19.0      0.0    0.0      0.0    0.0   \n",
       "OTU_5          2.0  53536.0    6.0    36.0    759.0   12.0  13795.0   77.0   \n",
       "OTU_47557      0.0      0.0    1.0     0.0      0.0    0.0      0.0    0.0   \n",
       "OTU_45150     69.0     23.0  202.0  1963.0      9.0    6.0      0.0   63.0   \n",
       "OTU_3        365.0   9387.0  671.0    95.0  29932.0    6.0    183.0  485.0   \n",
       "\n",
       "           724.O   991.O   ...    1319.O  1220.O  1436.O  1592.O  NTC8b  \\\n",
       "OTU_360      0.0     1.0   ...       0.0     0.0     0.0     0.0    0.0   \n",
       "OTU_5       25.0   489.0   ...       0.0     0.0     0.0     0.0    0.0   \n",
       "OTU_47557    0.0     0.0   ...       0.0     0.0     0.0     0.0    0.0   \n",
       "OTU_45150    0.0     1.0   ...       0.0     0.0     0.0     0.0    0.0   \n",
       "OTU_3      521.0  6685.0   ...       0.0     0.0     0.0     0.0    0.0   \n",
       "\n",
       "           1328.O  1413.O  1459.O  1454.O  1267.O  \n",
       "OTU_360       0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_5         0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_47557     0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_45150     0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_3         0.0     0.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 2688 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that df.append appends rows, not columns, so lets now filter along the observation\n",
    "# axis\n",
    "del filt_table, df\n",
    "filt_table = table.filter(otus[0:100],axis='observation',invert=False,inplace=False)\n",
    "df = filt_table.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74851\n"
     ]
    }
   ],
   "source": [
    "# now iterate thru, convert to DF and append\n",
    "num_otus = len(otus)\n",
    "print num_otus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000;\n",
    "num_iters = 50; #int(round(num_otus/chunk_size)) # 74\n",
    "print num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del filt_table\n",
    "# make first DF\n",
    "filt_table = table.filter(otus[0:chunk_size-1],axis='observation',invert=False,inplace=False)\n",
    "df_all = filt_table.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print len(df_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "curr_start = chunk_size\n",
    "for i in range(num_iters):\n",
    "    print curr_start\n",
    "    filt_table = table.filter(otus[curr_start:curr_start+chunk_size-1],axis='observation',invert=False,inplace=False)\n",
    "    df = filt_table.to_dataframe()\n",
    "    df_all = df_all.append(df)\n",
    "    curr_start = curr_start+chunk_size\n",
    "    del filt_table, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50949\n"
     ]
    }
   ],
   "source": [
    "print len(df_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# see also: https://stackoverflow.com/questions/17098654/how-to-store-a-dataframe-using-pandas\n",
    "# save as csv\n",
    "#outfile1 = \"ITS_otu1.csv\"\n",
    "outfile1 = fname.replace('biom','csv')\n",
    "df_all.to_csv(outfile1)\n",
    "del df_all\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# save as pickle\n",
    "outfile1 = 'ITS_otu1.pkl'\n",
    "df_all.to_pickle(outfile1)\n",
    "# to read: df = pd.read_pickle(outfile1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_all\n",
    "# second half\n",
    "filt_table = table.filter(otus[curr_start:curr_start+chunk_size-1],axis='observation',invert=False,inplace=False)\n",
    "df_all2 = filt_table.to_dataframe()\n",
    "del filt_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Column length mismatch: 2688 vs. 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-12fae4fa1574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mcurr_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilt_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0motus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcurr_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilt_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf_all2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcurr_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/melanie/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/biom/table.pyc\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3839\u001b[0m         \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3840\u001b[0m         df = pd.SparseDataFrame(mat, index=self.ids(axis='observation'),\n\u001b[0;32m-> 3841\u001b[0;31m                                 columns=self.ids())\n\u001b[0m\u001b[1;32m   3842\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/melanie/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/core/sparse/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, default_kind, default_fill_value, dtype, copy)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparseDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             mgr = self._init_mgr(data._data,\n",
      "\u001b[0;32m/home/melanie/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/core/sparse/frame.pyc\u001b[0m in \u001b[0;36m_init_matrix\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;34m\"\"\" Init self from ndarray or list of lists \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/melanie/anaconda2/envs/my_projects_env/lib/python2.7/site-packages/pandas/core/sparse/frame.pyc\u001b[0m in \u001b[0;36m_prep_index\u001b[0;34m(self, data, index, columns)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             raise ValueError('Column length mismatch: %d vs. %d' %\n\u001b[0;32m--> 220\u001b[0;31m                              (len(columns), K))\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             raise ValueError('Index length mismatch: %d vs. %d' %\n",
      "\u001b[0;31mValueError\u001b[0m: Column length mismatch: 2688 vs. 0"
     ]
    }
   ],
   "source": [
    "num_iters = 23\n",
    "curr_start = curr_start+chunk_size\n",
    "for i in range(num_iters):\n",
    "    print curr_start\n",
    "    filt_table = table.filter(otus[curr_start:curr_start+chunk_size-1],axis='observation',invert=False,inplace=False)\n",
    "    df = filt_table.to_dataframe()\n",
    "    df_all2 = df_all2.append(df)\n",
    "    curr_start = curr_start+chunk_size\n",
    "    del filt_table, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now add in remainder\n",
    "filt_table = table.filter(otus[curr_start:end],axis='observation',invert=False,inplace=False)\n",
    "df = filt_table.to_dataframe()\n",
    "del filt_table\n",
    "df_all2 = df_all2.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as csv \n",
    "#fname = 'ITS_otu_table_wTax.biom'\n",
    "#outfile = fname.replace('biom','csv')\n",
    "#outfile2 = 'ITS_otu2.csv'\n",
    "outfile2 = 'ITS_otu2.pkl'\n",
    "df_all2.to_pickle(outfile2)\n",
    "del df_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counts = [table.data(otu,axis='observation',dense=True) for otu in otus]\n",
    "#print type(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(counts[0],columns=['samples']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part_table = table[0,:] # first row; note: can only slice over 1 axis\n",
    "#part_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print part_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: the following line of code causes the notebook to crash if run\n",
    "# on the whole table; probably uses too much memory\n",
    "\n",
    "#df2 = exploding_panda(samp_table) \n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
