{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>574.O</th>\n",
       "      <th>1114.O</th>\n",
       "      <th>985.O</th>\n",
       "      <th>1112.O</th>\n",
       "      <th>700.O</th>\n",
       "      <th>264.O</th>\n",
       "      <th>410.O</th>\n",
       "      <th>29.O</th>\n",
       "      <th>22.O</th>\n",
       "      <th>798.O</th>\n",
       "      <th>...</th>\n",
       "      <th>1617.O</th>\n",
       "      <th>1489.O</th>\n",
       "      <th>807.O</th>\n",
       "      <th>1264.O</th>\n",
       "      <th>1180.O</th>\n",
       "      <th>1494.O</th>\n",
       "      <th>1480.O</th>\n",
       "      <th>1599.O</th>\n",
       "      <th>1604.O</th>\n",
       "      <th>1329.O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OTU_360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_5</th>\n",
       "      <td>679.0</td>\n",
       "      <td>2245.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>92555.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2330.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_47557</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_45150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTU_3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>9901.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3685.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>33480.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>578.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           574.O  1114.O  985.O   1112.O  700.O  264.O  410.O    29.O  \\\n",
       "OTU_360      0.0     0.0    0.0      0.0    0.0   57.0    0.0     0.0   \n",
       "OTU_5      679.0  2245.0  420.0  92555.0   19.0   16.0   16.0  4053.0   \n",
       "OTU_47557    0.0     0.0    1.0      0.0    0.0    0.0    0.0     0.0   \n",
       "OTU_45150    0.0     0.0   28.0    261.0    0.0    1.0    0.0     0.0   \n",
       "OTU_3       20.0  9901.0   38.0   3685.0   25.0   14.0    6.0    10.0   \n",
       "\n",
       "              22.O   798.O   ...    1617.O  1489.O  807.O  1264.O  1180.O  \\\n",
       "OTU_360        0.0     0.0   ...       0.0     0.0    0.0     0.0     0.0   \n",
       "OTU_5        950.0    19.0   ...      10.0     5.0   11.0     1.0   102.0   \n",
       "OTU_47557      0.0     0.0   ...       0.0     0.0    0.0     0.0     0.0   \n",
       "OTU_45150    432.0     3.0   ...       0.0     0.0    0.0     0.0     0.0   \n",
       "OTU_3      33480.0  2800.0   ...    1083.0     0.0  578.0     1.0    25.0   \n",
       "\n",
       "           1494.O  1480.O  1599.O  1604.O  1329.O  \n",
       "OTU_360       1.0     0.0     0.0     0.0     0.0  \n",
       "OTU_5         1.0    66.0    54.0  2330.0    39.0  \n",
       "OTU_47557     0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_45150     0.0     0.0     0.0     0.0     0.0  \n",
       "OTU_3         1.0    45.0   378.0  1842.0   137.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "os.chdir('../data/otu_tables_wTax')\n",
    "\n",
    "import biom\n",
    "from biom import parse_table, load_table\n",
    "fname = 'ITS_otu_table_wTax.biom'\n",
    "with open(fname) as f: \n",
    "    table = parse_table(f)\n",
    "    \n",
    "\n",
    "# randomly subsample without replacement\n",
    "num_sample = 100\n",
    "samp_table = table.subsample(num_sample,axis='sample',by_id=True)\n",
    "df = samp_table.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otus = [otu for otu in samp_table.ids(axis='observation')] # 'words'\n",
    "samples = [samp for samp in samp_table.ids()] # 'documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "otu_ids = list(xrange(len(otus)))  # indices for 'words' in the dictionary\n",
    "samp_ids = list(xrange(len(samples))) # indices for 'documents' in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "otu_ids = array(otu_ids)\n",
    "samp_ids = array(samp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(otu_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  6.79000000e+02,   2.24500000e+03,   4.20000000e+02, ...,\n",
       "          5.40000000e+01,   2.33000000e+03,   3.90000000e+01],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3004000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30040"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=len(otus)  # number of 'features' or 'words'\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=len(samples)  # number of 'samples' or 'articles'\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3004000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3004000\n"
     ]
    }
   ],
   "source": [
    "temp = df.values.reshape((n,m))\n",
    "print(temp.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "# construct at compressed sparse row format matrix, to represent the 'tf' matrix\n",
    "tf = csr_matrix(df.values.reshape((n,m))) #.toarray()\n",
    "#csr_matrix((df.values.reshape((n,m)), (samp_ids,otu_ids))) #.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x30040 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 176369 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t57.0\n",
      "  (0, 19)\t90.0\n",
      "  (0, 20)\t7.0\n",
      "  (0, 25)\t67.0\n",
      "  (0, 29)\t86.0\n",
      "  (0, 36)\t1.0\n",
      "  (0, 38)\t1.0\n",
      "  (0, 44)\t4.0\n",
      "  (0, 51)\t1.0\n",
      "  (0, 54)\t1.0\n",
      "  (0, 59)\t15.0\n",
      "  (0, 66)\t144.0\n",
      "  (0, 95)\t1.0\n",
      "  (0, 100)\t679.0\n",
      "  (0, 101)\t2245.0\n",
      "  (0, 102)\t420.0\n",
      "  (0, 103)\t92555.0\n",
      "  (0, 104)\t19.0\n",
      "  (0, 105)\t16.0\n",
      "  (0, 106)\t16.0\n",
      "  (0, 107)\t4053.0\n",
      "  (0, 108)\t950.0\n",
      "  (0, 109)\t19.0\n",
      "  (0, 110)\t2523.0\n",
      "  (0, 111)\t13.0\n",
      "  :\t:\n",
      "  (99, 27734)\t5.0\n",
      "  (99, 27835)\t5.0\n",
      "  (99, 27937)\t14.0\n",
      "  (99, 28034)\t3.0\n",
      "  (99, 28129)\t14.0\n",
      "  (99, 28232)\t5.0\n",
      "  (99, 28335)\t20.0\n",
      "  (99, 28438)\t2.0\n",
      "  (99, 28537)\t13.0\n",
      "  (99, 28629)\t4.0\n",
      "  (99, 28733)\t7.0\n",
      "  (99, 28834)\t5.0\n",
      "  (99, 28935)\t13.0\n",
      "  (99, 29030)\t1.0\n",
      "  (99, 29038)\t1.0\n",
      "  (99, 29134)\t2.0\n",
      "  (99, 29234)\t7.0\n",
      "  (99, 29334)\t4.0\n",
      "  (99, 29434)\t7.0\n",
      "  (99, 29537)\t11.0\n",
      "  (99, 29634)\t2.0\n",
      "  (99, 29732)\t7.0\n",
      "  (99, 29834)\t2.0\n",
      "  (99, 29929)\t3.0\n",
      "  (99, 30033)\t9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'OTU_360',\n",
       " 1: u'OTU_5',\n",
       " 2: u'OTU_47557',\n",
       " 3: u'OTU_45150',\n",
       " 4: u'OTU_3',\n",
       " 5: u'OTU_2689',\n",
       " 6: u'OTU_29537',\n",
       " 7: u'OTU_1345',\n",
       " 8: u'OTU_9',\n",
       " 9: u'OTU_5851',\n",
       " 10: u'OTU_53684',\n",
       " 11: u'OTU_37081',\n",
       " 12: u'OTU_57700',\n",
       " 13: u'OTU_42126',\n",
       " 14: u'OTU_57752',\n",
       " 15: u'OTU_8',\n",
       " 16: u'OTU_28173',\n",
       " 17: u'OTU_233',\n",
       " 18: u'OTU_64722',\n",
       " 19: u'OTU_4',\n",
       " 20: u'OTU_75815',\n",
       " 21: u'OTU_2',\n",
       " 22: u'OTU_79',\n",
       " 23: u'OTU_2803',\n",
       " 24: u'OTU_205',\n",
       " 25: u'OTU_386',\n",
       " 26: u'OTU_95',\n",
       " 27: u'OTU_17',\n",
       " 28: u'OTU_7',\n",
       " 29: u'OTU_327',\n",
       " 30: u'OTU_172',\n",
       " 31: u'OTU_18',\n",
       " 32: u'OTU_15',\n",
       " 33: u'OTU_36',\n",
       " 34: u'OTU_142',\n",
       " 35: u'OTU_664',\n",
       " 36: u'OTU_1',\n",
       " 37: u'OTU_22',\n",
       " 38: u'OTU_34',\n",
       " 39: u'OTU_5929',\n",
       " 40: u'OTU_330',\n",
       " 41: u'OTU_46',\n",
       " 42: u'OTU_56913',\n",
       " 43: u'OTU_123',\n",
       " 44: u'OTU_68612',\n",
       " 45: u'OTU_17232',\n",
       " 46: u'OTU_17793',\n",
       " 47: u'OTU_5017',\n",
       " 48: u'OTU_71422',\n",
       " 49: u'OTU_197',\n",
       " 50: u'OTU_7914',\n",
       " 51: u'OTU_61214',\n",
       " 52: u'OTU_63370',\n",
       " 53: u'OTU_85',\n",
       " 54: u'OTU_64499',\n",
       " 55: u'OTU_68',\n",
       " 56: u'OTU_47484',\n",
       " 57: u'OTU_15460',\n",
       " 58: u'OTU_157',\n",
       " 59: u'OTU_55683',\n",
       " 60: u'OTU_62',\n",
       " 61: u'OTU_69715',\n",
       " 62: u'OTU_99',\n",
       " 63: u'OTU_136',\n",
       " 64: u'OTU_47412',\n",
       " 65: u'OTU_291',\n",
       " 66: u'OTU_93',\n",
       " 67: u'OTU_1156',\n",
       " 68: u'OTU_2627',\n",
       " 69: u'OTU_706',\n",
       " 70: u'OTU_63030',\n",
       " 71: u'OTU_1906',\n",
       " 72: u'OTU_161',\n",
       " 73: u'OTU_75',\n",
       " 74: u'OTU_65483',\n",
       " 75: u'OTU_52937',\n",
       " 76: u'OTU_3479',\n",
       " 77: u'OTU_40686',\n",
       " 78: u'OTU_569',\n",
       " 79: u'OTU_938',\n",
       " 80: u'OTU_72026',\n",
       " 81: u'OTU_855',\n",
       " 82: u'OTU_24',\n",
       " 83: u'OTU_977',\n",
       " 84: u'OTU_145',\n",
       " 85: u'OTU_55695',\n",
       " 86: u'OTU_23',\n",
       " 87: u'OTU_69',\n",
       " 88: u'OTU_6975',\n",
       " 89: u'OTU_76063',\n",
       " 90: u'OTU_23668',\n",
       " 91: u'OTU_58005',\n",
       " 92: u'OTU_41415',\n",
       " 93: u'OTU_67256',\n",
       " 94: u'OTU_51497',\n",
       " 95: u'OTU_428',\n",
       " 96: u'OTU_4870',\n",
       " 97: u'OTU_2374',\n",
       " 98: u'OTU_40204',\n",
       " 99: u'OTU_110',\n",
       " 100: u'OTU_73171',\n",
       " 101: u'OTU_66146',\n",
       " 102: u'OTU_29637',\n",
       " 103: u'OTU_55980',\n",
       " 104: u'OTU_45139',\n",
       " 105: u'OTU_75841',\n",
       " 106: u'OTU_73154',\n",
       " 107: u'OTU_52263',\n",
       " 108: u'OTU_27',\n",
       " 109: u'OTU_198',\n",
       " 110: u'OTU_62597',\n",
       " 111: u'OTU_2530',\n",
       " 112: u'OTU_75377',\n",
       " 113: u'OTU_64',\n",
       " 114: u'OTU_21470',\n",
       " 115: u'OTU_129',\n",
       " 116: u'OTU_376',\n",
       " 117: u'OTU_37',\n",
       " 118: u'OTU_75723',\n",
       " 119: u'OTU_216',\n",
       " 120: u'OTU_74895',\n",
       " 121: u'OTU_74901',\n",
       " 122: u'OTU_6',\n",
       " 123: u'OTU_61881',\n",
       " 124: u'OTU_75650',\n",
       " 125: u'OTU_56992',\n",
       " 126: u'OTU_77',\n",
       " 127: u'OTU_5441',\n",
       " 128: u'OTU_30934',\n",
       " 129: u'OTU_150',\n",
       " 130: u'OTU_1373',\n",
       " 131: u'OTU_53907',\n",
       " 132: u'OTU_232',\n",
       " 133: u'OTU_6681',\n",
       " 134: u'OTU_39',\n",
       " 135: u'OTU_357',\n",
       " 136: u'OTU_31',\n",
       " 137: u'OTU_620',\n",
       " 138: u'OTU_46617',\n",
       " 139: u'OTU_74447',\n",
       " 140: u'OTU_230',\n",
       " 141: u'OTU_83',\n",
       " 142: u'OTU_33623',\n",
       " 143: u'OTU_61',\n",
       " 144: u'OTU_73655',\n",
       " 145: u'OTU_231',\n",
       " 146: u'OTU_87',\n",
       " 147: u'OTU_199',\n",
       " 148: u'OTU_65562',\n",
       " 149: u'OTU_444',\n",
       " 150: u'OTU_6141',\n",
       " 151: u'OTU_58977',\n",
       " 152: u'OTU_6248',\n",
       " 153: u'OTU_54',\n",
       " 154: u'OTU_75850',\n",
       " 155: u'OTU_1935',\n",
       " 156: u'OTU_10',\n",
       " 157: u'OTU_462',\n",
       " 158: u'OTU_285',\n",
       " 159: u'OTU_3574',\n",
       " 160: u'OTU_50982',\n",
       " 161: u'OTU_288',\n",
       " 162: u'OTU_2324',\n",
       " 163: u'OTU_57177',\n",
       " 164: u'OTU_60',\n",
       " 165: u'OTU_324',\n",
       " 166: u'OTU_264',\n",
       " 167: u'OTU_73',\n",
       " 168: u'OTU_44019',\n",
       " 169: u'OTU_73939',\n",
       " 170: u'OTU_2570',\n",
       " 171: u'OTU_439',\n",
       " 172: u'OTU_1411',\n",
       " 173: u'OTU_41763',\n",
       " 174: u'OTU_17642',\n",
       " 175: u'OTU_8374',\n",
       " 176: u'OTU_2210',\n",
       " 177: u'OTU_73348',\n",
       " 178: u'OTU_62726',\n",
       " 179: u'OTU_3530',\n",
       " 180: u'OTU_185',\n",
       " 181: u'OTU_108',\n",
       " 182: u'OTU_58644',\n",
       " 183: u'OTU_189',\n",
       " 184: u'OTU_92',\n",
       " 185: u'OTU_329',\n",
       " 186: u'OTU_75845',\n",
       " 187: u'OTU_1448',\n",
       " 188: u'OTU_54864',\n",
       " 189: u'OTU_527',\n",
       " 190: u'OTU_16',\n",
       " 191: u'OTU_46643',\n",
       " 192: u'OTU_65190',\n",
       " 193: u'OTU_55115',\n",
       " 194: u'OTU_19',\n",
       " 195: u'OTU_167',\n",
       " 196: u'OTU_62993',\n",
       " 197: u'OTU_125',\n",
       " 198: u'OTU_20',\n",
       " 199: u'OTU_184',\n",
       " 200: u'OTU_1034',\n",
       " 201: u'OTU_21660',\n",
       " 202: u'OTU_530',\n",
       " 203: u'OTU_240',\n",
       " 204: u'OTU_64994',\n",
       " 205: u'OTU_35081',\n",
       " 206: u'OTU_32',\n",
       " 207: u'OTU_188',\n",
       " 208: u'OTU_146',\n",
       " 209: u'OTU_24935',\n",
       " 210: u'OTU_74669',\n",
       " 211: u'OTU_29707',\n",
       " 212: u'OTU_626',\n",
       " 213: u'OTU_69678',\n",
       " 214: u'OTU_300',\n",
       " 215: u'OTU_68295',\n",
       " 216: u'OTU_72132',\n",
       " 217: u'OTU_62156',\n",
       " 218: u'OTU_1570',\n",
       " 219: u'OTU_74847',\n",
       " 220: u'OTU_29971',\n",
       " 221: u'OTU_75427',\n",
       " 222: u'OTU_3166',\n",
       " 223: u'OTU_51876',\n",
       " 224: u'OTU_14',\n",
       " 225: u'OTU_254',\n",
       " 226: u'OTU_501',\n",
       " 227: u'OTU_39903',\n",
       " 228: u'OTU_1239',\n",
       " 229: u'OTU_867',\n",
       " 230: u'OTU_53387',\n",
       " 231: u'OTU_71',\n",
       " 232: u'OTU_63428',\n",
       " 233: u'OTU_1736',\n",
       " 234: u'OTU_74798',\n",
       " 235: u'OTU_3322',\n",
       " 236: u'OTU_45967',\n",
       " 237: u'OTU_1574',\n",
       " 238: u'OTU_217',\n",
       " 239: u'OTU_43828',\n",
       " 240: u'OTU_67956',\n",
       " 241: u'OTU_35246',\n",
       " 242: u'OTU_67747',\n",
       " 243: u'OTU_8430',\n",
       " 244: u'OTU_69845',\n",
       " 245: u'OTU_4549',\n",
       " 246: u'OTU_38420',\n",
       " 247: u'OTU_103',\n",
       " 248: u'OTU_29320',\n",
       " 249: u'OTU_6872',\n",
       " 250: u'OTU_35478',\n",
       " 251: u'OTU_75656',\n",
       " 252: u'OTU_339',\n",
       " 253: u'OTU_21',\n",
       " 254: u'OTU_76058',\n",
       " 255: u'OTU_4739',\n",
       " 256: u'OTU_20211',\n",
       " 257: u'OTU_53',\n",
       " 258: u'OTU_292',\n",
       " 259: u'OTU_75752',\n",
       " 260: u'OTU_71225',\n",
       " 261: u'OTU_2767',\n",
       " 262: u'OTU_30602',\n",
       " 263: u'OTU_1628',\n",
       " 264: u'OTU_26316',\n",
       " 265: u'OTU_15341',\n",
       " 266: u'OTU_5666',\n",
       " 267: u'OTU_53871',\n",
       " 268: u'OTU_68428',\n",
       " 269: u'OTU_1052',\n",
       " 270: u'OTU_3411',\n",
       " 271: u'OTU_587',\n",
       " 272: u'OTU_50646',\n",
       " 273: u'OTU_1920',\n",
       " 274: u'OTU_766',\n",
       " 275: u'OTU_1425',\n",
       " 276: u'OTU_27637',\n",
       " 277: u'OTU_164',\n",
       " 278: u'OTU_64545',\n",
       " 279: u'OTU_56870',\n",
       " 280: u'OTU_342',\n",
       " 281: u'OTU_804',\n",
       " 282: u'OTU_52316',\n",
       " 283: u'OTU_73666',\n",
       " 284: u'OTU_270',\n",
       " 285: u'OTU_56469',\n",
       " 286: u'OTU_7234',\n",
       " 287: u'OTU_44009',\n",
       " 288: u'OTU_544',\n",
       " 289: u'OTU_33697',\n",
       " 290: u'OTU_194',\n",
       " 291: u'OTU_335',\n",
       " 292: u'OTU_16887',\n",
       " 293: u'OTU_283',\n",
       " 294: u'OTU_27253',\n",
       " 295: u'OTU_47052',\n",
       " 296: u'OTU_517',\n",
       " 297: u'OTU_72',\n",
       " 298: u'OTU_74',\n",
       " 299: u'OTU_316',\n",
       " 300: u'OTU_6784',\n",
       " 301: u'OTU_463',\n",
       " 302: u'OTU_35',\n",
       " 303: u'OTU_43917',\n",
       " 304: u'OTU_248',\n",
       " 305: u'OTU_46407',\n",
       " 306: u'OTU_223',\n",
       " 307: u'OTU_72065',\n",
       " 308: u'OTU_64983',\n",
       " 309: u'OTU_41',\n",
       " 310: u'OTU_71946',\n",
       " 311: u'OTU_455',\n",
       " 312: u'OTU_281',\n",
       " 313: u'OTU_319',\n",
       " 314: u'OTU_2216',\n",
       " 315: u'OTU_63163',\n",
       " 316: u'OTU_457',\n",
       " 317: u'OTU_71725',\n",
       " 318: u'OTU_111',\n",
       " 319: u'OTU_256',\n",
       " 320: u'OTU_64395',\n",
       " 321: u'OTU_51',\n",
       " 322: u'OTU_1723',\n",
       " 323: u'OTU_121',\n",
       " 324: u'OTU_193',\n",
       " 325: u'OTU_54718',\n",
       " 326: u'OTU_32422',\n",
       " 327: u'OTU_1576',\n",
       " 328: u'OTU_57755',\n",
       " 329: u'OTU_4596',\n",
       " 330: u'OTU_52014',\n",
       " 331: u'OTU_47774',\n",
       " 332: u'OTU_2085',\n",
       " 333: u'OTU_70939',\n",
       " 334: u'OTU_67675',\n",
       " 335: u'OTU_61730',\n",
       " 336: u'OTU_395',\n",
       " 337: u'OTU_508',\n",
       " 338: u'OTU_21008',\n",
       " 339: u'OTU_334',\n",
       " 340: u'OTU_141',\n",
       " 341: u'OTU_576',\n",
       " 342: u'OTU_64726',\n",
       " 343: u'OTU_1579',\n",
       " 344: u'OTU_1223',\n",
       " 345: u'OTU_69290',\n",
       " 346: u'OTU_60372',\n",
       " 347: u'OTU_847',\n",
       " 348: u'OTU_6279',\n",
       " 349: u'OTU_41796',\n",
       " 350: u'OTU_134',\n",
       " 351: u'OTU_397',\n",
       " 352: u'OTU_273',\n",
       " 353: u'OTU_4416',\n",
       " 354: u'OTU_494',\n",
       " 355: u'OTU_2175',\n",
       " 356: u'OTU_74494',\n",
       " 357: u'OTU_35014',\n",
       " 358: u'OTU_66476',\n",
       " 359: u'OTU_2246',\n",
       " 360: u'OTU_47474',\n",
       " 361: u'OTU_63820',\n",
       " 362: u'OTU_72223',\n",
       " 363: u'OTU_66091',\n",
       " 364: u'OTU_637',\n",
       " 365: u'OTU_46399',\n",
       " 366: u'OTU_1759',\n",
       " 367: u'OTU_2926',\n",
       " 368: u'OTU_67623',\n",
       " 369: u'OTU_5120',\n",
       " 370: u'OTU_73087',\n",
       " 371: u'OTU_3963',\n",
       " 372: u'OTU_46133',\n",
       " 373: u'OTU_797',\n",
       " 374: u'OTU_435',\n",
       " 375: u'OTU_55943',\n",
       " 376: u'OTU_580',\n",
       " 377: u'OTU_196',\n",
       " 378: u'OTU_58837',\n",
       " 379: u'OTU_49895',\n",
       " 380: u'OTU_151',\n",
       " 381: u'OTU_71424',\n",
       " 382: u'OTU_59710',\n",
       " 383: u'OTU_67',\n",
       " 384: u'OTU_236',\n",
       " 385: u'OTU_69740',\n",
       " 386: u'OTU_1840',\n",
       " 387: u'OTU_708',\n",
       " 388: u'OTU_72043',\n",
       " 389: u'OTU_70270',\n",
       " 390: u'OTU_310',\n",
       " 391: u'OTU_50615',\n",
       " 392: u'OTU_688',\n",
       " 393: u'OTU_45296',\n",
       " 394: u'OTU_26747',\n",
       " 395: u'OTU_304',\n",
       " 396: u'OTU_1872',\n",
       " 397: u'OTU_51169',\n",
       " 398: u'OTU_493',\n",
       " 399: u'OTU_2976',\n",
       " 400: u'OTU_190',\n",
       " 401: u'OTU_86',\n",
       " 402: u'OTU_26332',\n",
       " 403: u'OTU_29445',\n",
       " 404: u'OTU_63584',\n",
       " 405: u'OTU_1721',\n",
       " 406: u'OTU_10822',\n",
       " 407: u'OTU_57483',\n",
       " 408: u'OTU_39307',\n",
       " 409: u'OTU_400',\n",
       " 410: u'OTU_132',\n",
       " 411: u'OTU_74992',\n",
       " 412: u'OTU_62198',\n",
       " 413: u'OTU_47532',\n",
       " 414: u'OTU_258',\n",
       " 415: u'OTU_74214',\n",
       " 416: u'OTU_9972',\n",
       " 417: u'OTU_63006',\n",
       " 418: u'OTU_63171',\n",
       " 419: u'OTU_75271',\n",
       " 420: u'OTU_295',\n",
       " 421: u'OTU_1885',\n",
       " 422: u'OTU_114',\n",
       " 423: u'OTU_48019',\n",
       " 424: u'OTU_74254',\n",
       " 425: u'OTU_51393',\n",
       " 426: u'OTU_1626',\n",
       " 427: u'OTU_1963',\n",
       " 428: u'OTU_41327',\n",
       " 429: u'OTU_3041',\n",
       " 430: u'OTU_39309',\n",
       " 431: u'OTU_64852',\n",
       " 432: u'OTU_1296',\n",
       " 433: u'OTU_53858',\n",
       " 434: u'OTU_109',\n",
       " 435: u'OTU_307',\n",
       " 436: u'OTU_113',\n",
       " 437: u'OTU_65672',\n",
       " 438: u'OTU_29115',\n",
       " 439: u'OTU_65885',\n",
       " 440: u'OTU_375',\n",
       " 441: u'OTU_753',\n",
       " 442: u'OTU_650',\n",
       " 443: u'OTU_599',\n",
       " 444: u'OTU_27906',\n",
       " 445: u'OTU_1154',\n",
       " 446: u'OTU_50',\n",
       " 447: u'OTU_73141',\n",
       " 448: u'OTU_159',\n",
       " 449: u'OTU_49398',\n",
       " 450: u'OTU_18078',\n",
       " 451: u'OTU_46719',\n",
       " 452: u'OTU_1397',\n",
       " 453: u'OTU_2306',\n",
       " 454: u'OTU_177',\n",
       " 455: u'OTU_75817',\n",
       " 456: u'OTU_56',\n",
       " 457: u'OTU_70506',\n",
       " 458: u'OTU_67575',\n",
       " 459: u'OTU_445',\n",
       " 460: u'OTU_2314',\n",
       " 461: u'OTU_43283',\n",
       " 462: u'OTU_11',\n",
       " 463: u'OTU_28184',\n",
       " 464: u'OTU_34783',\n",
       " 465: u'OTU_71912',\n",
       " 466: u'OTU_58',\n",
       " 467: u'OTU_39154',\n",
       " 468: u'OTU_1315',\n",
       " 469: u'OTU_1620',\n",
       " 470: u'OTU_124',\n",
       " 471: u'OTU_67506',\n",
       " 472: u'OTU_18557',\n",
       " 473: u'OTU_600',\n",
       " 474: u'OTU_425',\n",
       " 475: u'OTU_7747',\n",
       " 476: u'OTU_26',\n",
       " 477: u'OTU_3516',\n",
       " 478: u'OTU_2871',\n",
       " 479: u'OTU_1495',\n",
       " 480: u'OTU_21875',\n",
       " 481: u'OTU_33544',\n",
       " 482: u'OTU_1010',\n",
       " 483: u'OTU_674',\n",
       " 484: u'OTU_52136',\n",
       " 485: u'OTU_75894',\n",
       " 486: u'OTU_69344',\n",
       " 487: u'OTU_2251',\n",
       " 488: u'OTU_66753',\n",
       " 489: u'OTU_3510',\n",
       " 490: u'OTU_186',\n",
       " 491: u'OTU_37938',\n",
       " 492: u'OTU_94',\n",
       " 493: u'OTU_52395',\n",
       " 494: u'OTU_59752',\n",
       " 495: u'OTU_32667',\n",
       " 496: u'OTU_641',\n",
       " 497: u'OTU_21968',\n",
       " 498: u'OTU_119',\n",
       " 499: u'OTU_53422',\n",
       " 500: u'OTU_255',\n",
       " 501: u'OTU_56879',\n",
       " 502: u'OTU_8383',\n",
       " 503: u'OTU_3403',\n",
       " 504: u'OTU_352',\n",
       " 505: u'OTU_393',\n",
       " 506: u'OTU_1102',\n",
       " 507: u'OTU_53653',\n",
       " 508: u'OTU_73234',\n",
       " 509: u'OTU_25237',\n",
       " 510: u'OTU_65922',\n",
       " 511: u'OTU_61566',\n",
       " 512: u'OTU_26997',\n",
       " 513: u'OTU_52948',\n",
       " 514: u'OTU_246',\n",
       " 515: u'OTU_821',\n",
       " 516: u'OTU_8577',\n",
       " 517: u'OTU_64941',\n",
       " 518: u'OTU_668',\n",
       " 519: u'OTU_23219',\n",
       " 520: u'OTU_74416',\n",
       " 521: u'OTU_32789',\n",
       " 522: u'OTU_1791',\n",
       " 523: u'OTU_191',\n",
       " 524: u'OTU_5933',\n",
       " 525: u'OTU_52602',\n",
       " 526: u'OTU_14020',\n",
       " 527: u'OTU_107',\n",
       " 528: u'OTU_2274',\n",
       " 529: u'OTU_220',\n",
       " 530: u'OTU_33',\n",
       " 531: u'OTU_46030',\n",
       " 532: u'OTU_551',\n",
       " 533: u'OTU_61593',\n",
       " 534: u'OTU_175',\n",
       " 535: u'OTU_63414',\n",
       " 536: u'OTU_71137',\n",
       " 537: u'OTU_471',\n",
       " 538: u'OTU_1391',\n",
       " 539: u'OTU_81',\n",
       " 540: u'OTU_71857',\n",
       " 541: u'OTU_54818',\n",
       " 542: u'OTU_970',\n",
       " 543: u'OTU_10512',\n",
       " 544: u'OTU_51783',\n",
       " 545: u'OTU_701',\n",
       " 546: u'OTU_2877',\n",
       " 547: u'OTU_75864',\n",
       " 548: u'OTU_22466',\n",
       " 549: u'OTU_61554',\n",
       " 550: u'OTU_314',\n",
       " 551: u'OTU_49554',\n",
       " 552: u'OTU_14939',\n",
       " 553: u'OTU_39324',\n",
       " 554: u'OTU_29346',\n",
       " 555: u'OTU_116',\n",
       " 556: u'OTU_622',\n",
       " 557: u'OTU_51270',\n",
       " 558: u'OTU_72860',\n",
       " 559: u'OTU_277',\n",
       " 560: u'OTU_46180',\n",
       " 561: u'OTU_183',\n",
       " 562: u'OTU_59377',\n",
       " 563: u'OTU_796',\n",
       " 564: u'OTU_74191',\n",
       " 565: u'OTU_73597',\n",
       " 566: u'OTU_18026',\n",
       " 567: u'OTU_70704',\n",
       " 568: u'OTU_3060',\n",
       " 569: u'OTU_322',\n",
       " 570: u'OTU_61068',\n",
       " 571: u'OTU_62787',\n",
       " 572: u'OTU_3224',\n",
       " 573: u'OTU_36229',\n",
       " 574: u'OTU_1378',\n",
       " 575: u'OTU_4738',\n",
       " 576: u'OTU_15974',\n",
       " 577: u'OTU_74278',\n",
       " 578: u'OTU_51815',\n",
       " 579: u'OTU_52264',\n",
       " 580: u'OTU_1772',\n",
       " 581: u'OTU_75411',\n",
       " 582: u'OTU_72809',\n",
       " 583: u'OTU_72688',\n",
       " 584: u'OTU_56783',\n",
       " 585: u'OTU_74172',\n",
       " 586: u'OTU_2143',\n",
       " 587: u'OTU_74967',\n",
       " 588: u'OTU_394',\n",
       " 589: u'OTU_625',\n",
       " 590: u'OTU_9233',\n",
       " 591: u'OTU_404',\n",
       " 592: u'OTU_23469',\n",
       " 593: u'OTU_59768',\n",
       " 594: u'OTU_52584',\n",
       " 595: u'OTU_44598',\n",
       " 596: u'OTU_214',\n",
       " 597: u'OTU_516',\n",
       " 598: u'OTU_75579',\n",
       " 599: u'OTU_29',\n",
       " 600: u'OTU_432',\n",
       " 601: u'OTU_5031',\n",
       " 602: u'OTU_41265',\n",
       " 603: u'OTU_338',\n",
       " 604: u'OTU_2565',\n",
       " 605: u'OTU_73144',\n",
       " 606: u'OTU_1259',\n",
       " 607: u'OTU_59125',\n",
       " 608: u'OTU_8040',\n",
       " 609: u'OTU_673',\n",
       " 610: u'OTU_29957',\n",
       " 611: u'OTU_2440',\n",
       " 612: u'OTU_621',\n",
       " 613: u'OTU_332',\n",
       " 614: u'OTU_6155',\n",
       " 615: u'OTU_8735',\n",
       " 616: u'OTU_1212',\n",
       " 617: u'OTU_1265',\n",
       " 618: u'OTU_69760',\n",
       " 619: u'OTU_32800',\n",
       " 620: u'OTU_15538',\n",
       " 621: u'OTU_683',\n",
       " 622: u'OTU_1837',\n",
       " 623: u'OTU_348',\n",
       " 624: u'OTU_68540',\n",
       " 625: u'OTU_62682',\n",
       " 626: u'OTU_57305',\n",
       " 627: u'OTU_55895',\n",
       " 628: u'OTU_24818',\n",
       " 629: u'OTU_6190',\n",
       " 630: u'OTU_47181',\n",
       " 631: u'OTU_62481',\n",
       " 632: u'OTU_47573',\n",
       " 633: u'OTU_63',\n",
       " 634: u'OTU_47253',\n",
       " 635: u'OTU_69410',\n",
       " 636: u'OTU_41892',\n",
       " 637: u'OTU_65',\n",
       " 638: u'OTU_58784',\n",
       " 639: u'OTU_43046',\n",
       " 640: u'OTU_74631',\n",
       " 641: u'OTU_31331',\n",
       " 642: u'OTU_1096',\n",
       " 643: u'OTU_40868',\n",
       " 644: u'OTU_51899',\n",
       " 645: u'OTU_48006',\n",
       " 646: u'OTU_174',\n",
       " 647: u'OTU_227',\n",
       " 648: u'OTU_44577',\n",
       " 649: u'OTU_1230',\n",
       " 650: u'OTU_64643',\n",
       " 651: u'OTU_2162',\n",
       " 652: u'OTU_2151',\n",
       " 653: u'OTU_70439',\n",
       " 654: u'OTU_25',\n",
       " 655: u'OTU_70732',\n",
       " 656: u'OTU_62986',\n",
       " 657: u'OTU_64502',\n",
       " 658: u'OTU_52966',\n",
       " 659: u'OTU_552',\n",
       " 660: u'OTU_2223',\n",
       " 661: u'OTU_434',\n",
       " 662: u'OTU_73873',\n",
       " 663: u'OTU_55',\n",
       " 664: u'OTU_73425',\n",
       " 665: u'OTU_170',\n",
       " 666: u'OTU_5335',\n",
       " 667: u'OTU_38906',\n",
       " 668: u'OTU_66330',\n",
       " 669: u'OTU_74165',\n",
       " 670: u'OTU_75735',\n",
       " 671: u'OTU_66734',\n",
       " 672: u'OTU_65057',\n",
       " 673: u'OTU_208',\n",
       " 674: u'OTU_4627',\n",
       " 675: u'OTU_35602',\n",
       " 676: u'OTU_299',\n",
       " 677: u'OTU_618',\n",
       " 678: u'OTU_39257',\n",
       " 679: u'OTU_56898',\n",
       " 680: u'OTU_63588',\n",
       " 681: u'OTU_70676',\n",
       " 682: u'OTU_73527',\n",
       " 683: u'OTU_130',\n",
       " 684: u'OTU_54825',\n",
       " 685: u'OTU_71753',\n",
       " 686: u'OTU_382',\n",
       " 687: u'OTU_373',\n",
       " 688: u'OTU_59',\n",
       " 689: u'OTU_57344',\n",
       " 690: u'OTU_63433',\n",
       " 691: u'OTU_67975',\n",
       " 692: u'OTU_29259',\n",
       " 693: u'OTU_75153',\n",
       " 694: u'OTU_35540',\n",
       " 695: u'OTU_65674',\n",
       " 696: u'OTU_6184',\n",
       " 697: u'OTU_58397',\n",
       " 698: u'OTU_660',\n",
       " 699: u'OTU_53469',\n",
       " 700: u'OTU_450',\n",
       " 701: u'OTU_5661',\n",
       " 702: u'OTU_353',\n",
       " 703: u'OTU_924',\n",
       " 704: u'OTU_10527',\n",
       " 705: u'OTU_30860',\n",
       " 706: u'OTU_43009',\n",
       " 707: u'OTU_72169',\n",
       " 708: u'OTU_68794',\n",
       " 709: u'OTU_648',\n",
       " 710: u'OTU_43101',\n",
       " 711: u'OTU_4439',\n",
       " 712: u'OTU_331',\n",
       " 713: u'OTU_67151',\n",
       " 714: u'OTU_67888',\n",
       " 715: u'OTU_28492',\n",
       " 716: u'OTU_405',\n",
       " 717: u'OTU_50021',\n",
       " 718: u'OTU_52782',\n",
       " 719: u'OTU_36221',\n",
       " 720: u'OTU_74213',\n",
       " 721: u'OTU_985',\n",
       " 722: u'OTU_61039',\n",
       " 723: u'OTU_4435',\n",
       " 724: u'OTU_66412',\n",
       " 725: u'OTU_75960',\n",
       " 726: u'OTU_75457',\n",
       " 727: u'OTU_1386',\n",
       " 728: u'OTU_4160',\n",
       " 729: u'OTU_43185',\n",
       " 730: u'OTU_76156',\n",
       " 731: u'OTU_355',\n",
       " 732: u'OTU_74232',\n",
       " 733: u'OTU_44',\n",
       " 734: u'OTU_66057',\n",
       " 735: u'OTU_24204',\n",
       " 736: u'OTU_953',\n",
       " 737: u'OTU_74923',\n",
       " 738: u'OTU_33264',\n",
       " 739: u'OTU_69223',\n",
       " 740: u'OTU_48152',\n",
       " 741: u'OTU_13017',\n",
       " 742: u'OTU_75714',\n",
       " 743: u'OTU_45753',\n",
       " 744: u'OTU_63766',\n",
       " 745: u'OTU_612',\n",
       " 746: u'OTU_61042',\n",
       " 747: u'OTU_30076',\n",
       " 748: u'OTU_2902',\n",
       " 749: u'OTU_872',\n",
       " 750: u'OTU_29360',\n",
       " 751: u'OTU_603',\n",
       " 752: u'OTU_2525',\n",
       " 753: u'OTU_402',\n",
       " 754: u'OTU_34791',\n",
       " 755: u'OTU_4621',\n",
       " 756: u'OTU_385',\n",
       " 757: u'OTU_4574',\n",
       " 758: u'OTU_71707',\n",
       " 759: u'OTU_430',\n",
       " 760: u'OTU_43039',\n",
       " 761: u'OTU_206',\n",
       " 762: u'OTU_773',\n",
       " 763: u'OTU_8240',\n",
       " 764: u'OTU_165',\n",
       " 765: u'OTU_481',\n",
       " 766: u'OTU_684',\n",
       " 767: u'OTU_607',\n",
       " 768: u'OTU_2327',\n",
       " 769: u'OTU_475',\n",
       " 770: u'OTU_57898',\n",
       " 771: u'OTU_15343',\n",
       " 772: u'OTU_45493',\n",
       " 773: u'OTU_57',\n",
       " 774: u'OTU_70418',\n",
       " 775: u'OTU_1704',\n",
       " 776: u'OTU_586',\n",
       " 777: u'OTU_67930',\n",
       " 778: u'OTU_265',\n",
       " 779: u'OTU_10581',\n",
       " 780: u'OTU_1530',\n",
       " 781: u'OTU_23385',\n",
       " 782: u'OTU_45290',\n",
       " 783: u'OTU_794',\n",
       " 784: u'OTU_59421',\n",
       " 785: u'OTU_67018',\n",
       " 786: u'OTU_42',\n",
       " 787: u'OTU_10154',\n",
       " 788: u'OTU_485',\n",
       " 789: u'OTU_32782',\n",
       " 790: u'OTU_46720',\n",
       " 791: u'OTU_1313',\n",
       " 792: u'OTU_61496',\n",
       " 793: u'OTU_1217',\n",
       " 794: u'OTU_1371',\n",
       " 795: u'OTU_24354',\n",
       " 796: u'OTU_23167',\n",
       " 797: u'OTU_12751',\n",
       " 798: u'OTU_60566',\n",
       " 799: u'OTU_11697',\n",
       " 800: u'OTU_2366',\n",
       " 801: u'OTU_89',\n",
       " 802: u'OTU_57806',\n",
       " 803: u'OTU_38024',\n",
       " 804: u'OTU_34735',\n",
       " 805: u'OTU_24907',\n",
       " 806: u'OTU_6058',\n",
       " 807: u'OTU_417',\n",
       " 808: u'OTU_137',\n",
       " 809: u'OTU_26199',\n",
       " 810: u'OTU_2414',\n",
       " 811: u'OTU_32327',\n",
       " 812: u'OTU_11237',\n",
       " 813: u'OTU_842',\n",
       " 814: u'OTU_399',\n",
       " 815: u'OTU_47236',\n",
       " 816: u'OTU_69892',\n",
       " 817: u'OTU_72941',\n",
       " 818: u'OTU_75245',\n",
       " 819: u'OTU_2294',\n",
       " 820: u'OTU_36590',\n",
       " 821: u'OTU_1168',\n",
       " 822: u'OTU_69636',\n",
       " 823: u'OTU_5395',\n",
       " 824: u'OTU_61745',\n",
       " 825: u'OTU_73192',\n",
       " 826: u'OTU_410',\n",
       " 827: u'OTU_754',\n",
       " 828: u'OTU_16981',\n",
       " 829: u'OTU_453',\n",
       " 830: u'OTU_63075',\n",
       " 831: u'OTU_715',\n",
       " 832: u'OTU_47810',\n",
       " 833: u'OTU_984',\n",
       " 834: u'OTU_68245',\n",
       " 835: u'OTU_41950',\n",
       " 836: u'OTU_44586',\n",
       " 837: u'OTU_58731',\n",
       " 838: u'OTU_7822',\n",
       " 839: u'OTU_63597',\n",
       " 840: u'OTU_53405',\n",
       " 841: u'OTU_41521',\n",
       " 842: u'OTU_64977',\n",
       " 843: u'OTU_703',\n",
       " 844: u'OTU_8182',\n",
       " 845: u'OTU_35029',\n",
       " 846: u'OTU_55795',\n",
       " 847: u'OTU_693',\n",
       " 848: u'OTU_3577',\n",
       " 849: u'OTU_49145',\n",
       " 850: u'OTU_965',\n",
       " 851: u'OTU_282',\n",
       " 852: u'OTU_2770',\n",
       " 853: u'OTU_3792',\n",
       " 854: u'OTU_166',\n",
       " 855: u'OTU_75785',\n",
       " 856: u'OTU_51095',\n",
       " 857: u'OTU_29051',\n",
       " 858: u'OTU_72305',\n",
       " 859: u'OTU_10708',\n",
       " 860: u'OTU_42365',\n",
       " 861: u'OTU_951',\n",
       " 862: u'OTU_745',\n",
       " 863: u'OTU_49015',\n",
       " 864: u'OTU_126',\n",
       " 865: u'OTU_63815',\n",
       " 866: u'OTU_1278',\n",
       " 867: u'OTU_974',\n",
       " 868: u'OTU_2884',\n",
       " 869: u'OTU_74479',\n",
       " 870: u'OTU_69953',\n",
       " 871: u'OTU_2273',\n",
       " 872: u'OTU_71632',\n",
       " 873: u'OTU_102',\n",
       " 874: u'OTU_75951',\n",
       " 875: u'OTU_2443',\n",
       " 876: u'OTU_341',\n",
       " 877: u'OTU_47844',\n",
       " 878: u'OTU_1305',\n",
       " 879: u'OTU_72760',\n",
       " 880: u'OTU_1022',\n",
       " 881: u'OTU_17135',\n",
       " 882: u'OTU_10117',\n",
       " 883: u'OTU_70550',\n",
       " 884: u'OTU_49056',\n",
       " 885: u'OTU_6827',\n",
       " 886: u'OTU_72271',\n",
       " 887: u'OTU_50770',\n",
       " 888: u'OTU_879',\n",
       " 889: u'OTU_902',\n",
       " 890: u'OTU_477',\n",
       " 891: u'OTU_74047',\n",
       " 892: u'OTU_268',\n",
       " 893: u'OTU_54666',\n",
       " 894: u'OTU_41039',\n",
       " 895: u'OTU_2943',\n",
       " 896: u'OTU_43040',\n",
       " 897: u'OTU_52483',\n",
       " 898: u'OTU_63711',\n",
       " 899: u'OTU_2170',\n",
       " 900: u'OTU_886',\n",
       " 901: u'OTU_63143',\n",
       " 902: u'OTU_2093',\n",
       " 903: u'OTU_55739',\n",
       " 904: u'OTU_1447',\n",
       " 905: u'OTU_72552',\n",
       " 906: u'OTU_80',\n",
       " 907: u'OTU_48611',\n",
       " 908: u'OTU_73944',\n",
       " 909: u'OTU_1529',\n",
       " 910: u'OTU_1033',\n",
       " 911: u'OTU_1413',\n",
       " 912: u'OTU_48603',\n",
       " 913: u'OTU_71304',\n",
       " 914: u'OTU_51424',\n",
       " 915: u'OTU_55701',\n",
       " 916: u'OTU_72113',\n",
       " 917: u'OTU_59284',\n",
       " 918: u'OTU_56864',\n",
       " 919: u'OTU_73510',\n",
       " 920: u'OTU_74138',\n",
       " 921: u'OTU_318',\n",
       " 922: u'OTU_58427',\n",
       " 923: u'OTU_51736',\n",
       " 924: u'OTU_73149',\n",
       " 925: u'OTU_593',\n",
       " 926: u'OTU_1162',\n",
       " 927: u'OTU_71360',\n",
       " 928: u'OTU_75649',\n",
       " 929: u'OTU_45291',\n",
       " 930: u'OTU_71877',\n",
       " 931: u'OTU_105',\n",
       " 932: u'OTU_59729',\n",
       " 933: u'OTU_14531',\n",
       " 934: u'OTU_514',\n",
       " 935: u'OTU_158',\n",
       " 936: u'OTU_44333',\n",
       " 937: u'OTU_187',\n",
       " 938: u'OTU_47',\n",
       " 939: u'OTU_66960',\n",
       " 940: u'OTU_336',\n",
       " 941: u'OTU_10050',\n",
       " 942: u'OTU_28688',\n",
       " 943: u'OTU_70419',\n",
       " 944: u'OTU_594',\n",
       " 945: u'OTU_73448',\n",
       " 946: u'OTU_11171',\n",
       " 947: u'OTU_8873',\n",
       " 948: u'OTU_21647',\n",
       " 949: u'OTU_63300',\n",
       " 950: u'OTU_6158',\n",
       " 951: u'OTU_7624',\n",
       " 952: u'OTU_60065',\n",
       " 953: u'OTU_104',\n",
       " 954: u'OTU_17733',\n",
       " 955: u'OTU_67563',\n",
       " 956: u'OTU_44971',\n",
       " 957: u'OTU_122',\n",
       " 958: u'OTU_244',\n",
       " 959: u'OTU_1835',\n",
       " 960: u'OTU_38980',\n",
       " 961: u'OTU_63262',\n",
       " 962: u'OTU_38286',\n",
       " 963: u'OTU_45309',\n",
       " 964: u'OTU_23496',\n",
       " 965: u'OTU_7073',\n",
       " 966: u'OTU_669',\n",
       " 967: u'OTU_945',\n",
       " 968: u'OTU_1290',\n",
       " 969: u'OTU_60762',\n",
       " 970: u'OTU_642',\n",
       " 971: u'OTU_59367',\n",
       " 972: u'OTU_722',\n",
       " 973: u'OTU_317',\n",
       " 974: u'OTU_337',\n",
       " 975: u'OTU_46178',\n",
       " 976: u'OTU_60292',\n",
       " 977: u'OTU_4995',\n",
       " 978: u'OTU_75362',\n",
       " 979: u'OTU_65536',\n",
       " 980: u'OTU_1139',\n",
       " 981: u'OTU_18595',\n",
       " 982: u'OTU_16448',\n",
       " 983: u'OTU_56495',\n",
       " 984: u'OTU_51682',\n",
       " 985: u'OTU_33355',\n",
       " 986: u'OTU_43891',\n",
       " 987: u'OTU_7467',\n",
       " 988: u'OTU_431',\n",
       " 989: u'OTU_148',\n",
       " 990: u'OTU_67968',\n",
       " 991: u'OTU_374',\n",
       " 992: u'OTU_369',\n",
       " 993: u'OTU_1429',\n",
       " 994: u'OTU_2384',\n",
       " 995: u'OTU_51225',\n",
       " 996: u'OTU_398',\n",
       " 997: u'OTU_1128',\n",
       " 998: u'OTU_69119',\n",
       " 999: u'OTU_52426',\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary, i.e. mapping from 'word IDs' to 'words'\n",
    "vocab = dict(zip(otu_ids, otus))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = n\n",
    "n_features = 1000 #m\n",
    "n_components = 100 # number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "\"\"\"\n",
    "max_df: float in range [0,1], default=1.0. when building vocab ignore terms that have a \n",
    "    document freq strictly higher than the given threshold\n",
    "min_df: float in range [0,1], default=1.0. ignore terms in doc with freq lower than given\n",
    "    threshold\n",
    "vocabulary: mapping or iterable, optional.Either a mapping (e.g. a dict) where keys are \n",
    "    terms and values are indices in the feature matrix, or an iterable over terms. if not given,\n",
    "    a vocabulary is determined from the input documents.\n",
    "\"\"\"\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=1.0, min_df=0.0,max_features=n_features,\n",
    "                               vocabulary=list(set(vocab)))\n",
    "#tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "#                                max_features=n_features)\n",
    "#t0 = time()\n",
    "#tf = tf_vectorizer.fit_transform(data_samples)\n",
    "#print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CountVectorizer in module sklearn.feature_extraction.text object:\n",
      "\n",
      "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer=u'word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>)\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([str(vocab[feature_names[i]])\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        #message += \" \".join([str(feature_names[i])\n",
    "        #                     for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=100 and n_features=1000...\n",
      "done in 13.757s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: OTU_37847 OTU_31409 OTU_74704 OTU_58532 OTU_63353 OTU_24139 OTU_49978 OTU_58890 OTU_61107 OTU_4153 OTU_73109 OTU_8855 OTU_2414 OTU_67825 OTU_54182 OTU_68518 OTU_5178 OTU_61143 OTU_9398 OTU_60040\n",
      "Topic #1: OTU_40474 OTU_43658 OTU_13065 OTU_20512 OTU_5646 OTU_74889 OTU_34754 OTU_3430 OTU_3467 OTU_45005 OTU_21272 OTU_74457 OTU_36622 OTU_22882 OTU_3638 OTU_42370 OTU_58520 OTU_33680 OTU_20101 OTU_3325\n",
      "Topic #2: OTU_47475 OTU_63145 OTU_27595 OTU_57844 OTU_63354 OTU_275 OTU_49617 OTU_38165 OTU_69425 OTU_58589 OTU_32447 OTU_28499 OTU_48222 OTU_6983 OTU_57859 OTU_15680 OTU_50635 OTU_4062 OTU_62747 OTU_8658\n",
      "Topic #3: OTU_501 OTU_46256 OTU_75782 OTU_22874 OTU_4237 OTU_76107 OTU_275 OTU_60332 OTU_6724 OTU_70196 OTU_8658 OTU_63141 OTU_20106 OTU_6763 OTU_40657 OTU_68332 OTU_49271 OTU_55980 OTU_59039 OTU_31700\n",
      "Topic #4: OTU_275 OTU_8658 OTU_68332 OTU_55980 OTU_75078 OTU_1436 OTU_1979 OTU_28554 OTU_3243 OTU_23749 OTU_14200 OTU_75889 OTU_39132 OTU_2147 OTU_345 OTU_70 OTU_46948 OTU_39309 OTU_70860 OTU_61881\n",
      "Topic #5: OTU_275 OTU_8658 OTU_68332 OTU_28554 OTU_55980 OTU_46256 OTU_14200 OTU_75078 OTU_45601 OTU_3243 OTU_1979 OTU_38151 OTU_46948 OTU_345 OTU_4775 OTU_70 OTU_1436 OTU_47181 OTU_39132 OTU_2147\n",
      "Topic #6: OTU_275 OTU_8658 OTU_68332 OTU_28554 OTU_1436 OTU_55980 OTU_345 OTU_14200 OTU_1979 OTU_46948 OTU_70860 OTU_75078 OTU_23749 OTU_2147 OTU_75889 OTU_3243 OTU_45601 OTU_48910 OTU_39309 OTU_39132\n",
      "Topic #7: OTU_5238 OTU_8063 OTU_56514 OTU_18184 OTU_501 OTU_18565 OTU_54041 OTU_19748 OTU_21330 OTU_54081 OTU_74302 OTU_3358 OTU_5070 OTU_5390 OTU_22874 OTU_3789 OTU_46256 OTU_53127 OTU_44756 OTU_50491\n",
      "Topic #8: OTU_8658 OTU_275 OTU_28554 OTU_14200 OTU_68332 OTU_75078 OTU_46948 OTU_1979 OTU_1436 OTU_75889 OTU_54217 OTU_45601 OTU_70 OTU_37847 OTU_39309 OTU_345 OTU_3243 OTU_2147 OTU_69227 OTU_34315\n",
      "Topic #9: OTU_75774 OTU_7527 OTU_36124 OTU_6317 OTU_68653 OTU_61685 OTU_11610 OTU_70980 OTU_4738 OTU_66493 OTU_20106 OTU_4132 OTU_27307 OTU_43328 OTU_56187 OTU_17633 OTU_55215 OTU_44404 OTU_6506 OTU_6054\n",
      "Topic #10: OTU_3169 OTU_275 OTU_37553 OTU_23404 OTU_68010 OTU_58431 OTU_21877 OTU_8658 OTU_28554 OTU_58931 OTU_71019 OTU_55980 OTU_22749 OTU_75078 OTU_501 OTU_14200 OTU_68332 OTU_16274 OTU_56951 OTU_1436\n",
      "Topic #11: OTU_5697 OTU_881 OTU_6385 OTU_472 OTU_14584 OTU_7532 OTU_23668 OTU_437 OTU_66555 OTU_38841 OTU_307 OTU_50072 OTU_52696 OTU_37836 OTU_605 OTU_27039 OTU_29184 OTU_4878 OTU_63928 OTU_52983\n",
      "Topic #12: OTU_275 OTU_28554 OTU_8658 OTU_55980 OTU_1436 OTU_68332 OTU_14200 OTU_75078 OTU_1979 OTU_2147 OTU_70 OTU_39309 OTU_23749 OTU_75889 OTU_38151 OTU_3243 OTU_46948 OTU_345 OTU_70860 OTU_1373\n",
      "Topic #13: OTU_5496 OTU_2861 OTU_46973 OTU_26344 OTU_61984 OTU_41208 OTU_4237 OTU_12670 OTU_50895 OTU_50588 OTU_65174 OTU_74216 OTU_37747 OTU_4051 OTU_28092 OTU_51105 OTU_74374 OTU_10623 OTU_48204 OTU_5270\n",
      "Topic #14: OTU_275 OTU_68332 OTU_8658 OTU_28554 OTU_55980 OTU_1979 OTU_75078 OTU_2147 OTU_46948 OTU_75889 OTU_70 OTU_345 OTU_54217 OTU_1436 OTU_14200 OTU_23749 OTU_70860 OTU_5380 OTU_3243 OTU_39309\n",
      "Topic #15: OTU_43330 OTU_26693 OTU_275 OTU_74678 OTU_8658 OTU_55980 OTU_68332 OTU_74956 OTU_28554 OTU_2599 OTU_46948 OTU_14200 OTU_75889 OTU_37847 OTU_47181 OTU_1482 OTU_1436 OTU_3243 OTU_65690 OTU_39132\n",
      "Topic #16: OTU_69410 OTU_45967 OTU_22750 OTU_275 OTU_8658 OTU_74395 OTU_74371 OTU_44586 OTU_28554 OTU_3662 OTU_444 OTU_52112 OTU_55980 OTU_6213 OTU_30329 OTU_1132 OTU_14200 OTU_2873 OTU_46948 OTU_68332\n",
      "Topic #17: OTU_25527 OTU_52014 OTU_50408 OTU_37075 OTU_1723 OTU_45315 OTU_60131 OTU_29676 OTU_771 OTU_31756 OTU_275 OTU_50081 OTU_8658 OTU_6834 OTU_23291 OTU_68332 OTU_5526 OTU_28554 OTU_15461 OTU_35015\n",
      "Topic #18: OTU_46554 OTU_25657 OTU_20231 OTU_51064 OTU_19015 OTU_2661 OTU_275 OTU_11228 OTU_14713 OTU_3789 OTU_58737 OTU_53098 OTU_21867 OTU_50998 OTU_57872 OTU_5018 OTU_11915 OTU_71067 OTU_2544 OTU_74545\n",
      "Topic #19: OTU_22065 OTU_51985 OTU_39330 OTU_69982 OTU_10487 OTU_13736 OTU_49045 OTU_33628 OTU_7996 OTU_9956 OTU_3540 OTU_70599 OTU_68031 OTU_68719 OTU_4436 OTU_58108 OTU_41410 OTU_3969 OTU_51873 OTU_16204\n",
      "Topic #20: OTU_70201 OTU_28610 OTU_7194 OTU_275 OTU_8658 OTU_51073 OTU_68332 OTU_28554 OTU_46948 OTU_55980 OTU_10576 OTU_1979 OTU_70 OTU_14200 OTU_3243 OTU_68617 OTU_1436 OTU_44038 OTU_43255 OTU_23749\n",
      "Topic #21: OTU_6248 OTU_56267 OTU_275 OTU_8658 OTU_2932 OTU_66852 OTU_63884 OTU_55980 OTU_68332 OTU_32774 OTU_807 OTU_28554 OTU_61199 OTU_24858 OTU_64747 OTU_14200 OTU_46948 OTU_70860 OTU_75078 OTU_67982\n",
      "Topic #22: OTU_275 OTU_8658 OTU_28554 OTU_55980 OTU_68332 OTU_46948 OTU_75078 OTU_1436 OTU_14200 OTU_3243 OTU_345 OTU_23749 OTU_54217 OTU_47181 OTU_75889 OTU_1979 OTU_45601 OTU_70860 OTU_70 OTU_39309\n",
      "Topic #23: OTU_64138 OTU_501 OTU_275 OTU_8658 OTU_55980 OTU_68332 OTU_22874 OTU_28554 OTU_1436 OTU_76107 OTU_1979 OTU_60332 OTU_31700 OTU_46948 OTU_70 OTU_40657 OTU_20106 OTU_14200 OTU_6763 OTU_63141\n",
      "Topic #24: OTU_369 OTU_29921 OTU_4381 OTU_69468 OTU_1142 OTU_639 OTU_73605 OTU_30899 OTU_65319 OTU_1348 OTU_66658 OTU_8658 OTU_7115 OTU_275 OTU_46554 OTU_11254 OTU_46256 OTU_19735 OTU_5560 OTU_28554\n",
      "Topic #25: OTU_69867 OTU_49409 OTU_3925 OTU_67751 OTU_23397 OTU_1408 OTU_48589 OTU_58907 OTU_69106 OTU_37847 OTU_36079 OTU_68419 OTU_62016 OTU_6551 OTU_72735 OTU_62454 OTU_36482 OTU_606 OTU_42925 OTU_16790\n",
      "Topic #26: OTU_75140 OTU_36825 OTU_275 OTU_8658 OTU_55980 OTU_28554 OTU_74538 OTU_68332 OTU_75078 OTU_14200 OTU_1979 OTU_46948 OTU_57297 OTU_23749 OTU_43330 OTU_28322 OTU_39309 OTU_1436 OTU_2624 OTU_54217\n",
      "Topic #27: OTU_275 OTU_8658 OTU_55980 OTU_68332 OTU_606 OTU_1979 OTU_28554 OTU_75889 OTU_1436 OTU_70 OTU_75078 OTU_3243 OTU_47181 OTU_75158 OTU_4775 OTU_46948 OTU_57302 OTU_345 OTU_39309 OTU_70860\n",
      "Topic #28: OTU_275 OTU_8658 OTU_55980 OTU_28554 OTU_1436 OTU_68332 OTU_345 OTU_46948 OTU_49527 OTU_23749 OTU_60901 OTU_14200 OTU_44404 OTU_70860 OTU_75889 OTU_38151 OTU_54217 OTU_2147 OTU_3243 OTU_75078\n",
      "Topic #29: OTU_64138 OTU_275 OTU_8658 OTU_23681 OTU_68332 OTU_28554 OTU_55980 OTU_4589 OTU_75078 OTU_1436 OTU_16572 OTU_14200 OTU_54217 OTU_345 OTU_46948 OTU_501 OTU_34315 OTU_28899 OTU_39309 OTU_1979\n",
      "Topic #30: OTU_11461 OTU_34498 OTU_46256 OTU_56514 OTU_65007 OTU_275 OTU_8658 OTU_68332 OTU_18184 OTU_270 OTU_75782 OTU_60582 OTU_3794 OTU_18565 OTU_55980 OTU_345 OTU_28554 OTU_28095 OTU_18762 OTU_3243\n",
      "Topic #31: OTU_64138 OTU_275 OTU_8658 OTU_28554 OTU_55980 OTU_68332 OTU_14200 OTU_1436 OTU_23749 OTU_1979 OTU_70 OTU_345 OTU_3243 OTU_75889 OTU_2147 OTU_46948 OTU_45601 OTU_39309 OTU_70860 OTU_61881\n",
      "Topic #32: OTU_32220 OTU_61019 OTU_14694 OTU_45624 OTU_4511 OTU_15289 OTU_51259 OTU_7327 OTU_66811 OTU_37847 OTU_26199 OTU_40862 OTU_19637 OTU_60872 OTU_3334 OTU_75015 OTU_33061 OTU_70340 OTU_74165 OTU_64955\n",
      "Topic #33: OTU_33424 OTU_55044 OTU_1076 OTU_75064 OTU_22995 OTU_3363 OTU_14614 OTU_440 OTU_13979 OTU_10019 OTU_48859 OTU_417 OTU_275 OTU_47589 OTU_10631 OTU_55525 OTU_32225 OTU_501 OTU_1182 OTU_6124\n",
      "Topic #34: OTU_13306 OTU_652 OTU_18252 OTU_26694 OTU_61313 OTU_59843 OTU_38504 OTU_16589 OTU_9624 OTU_4805 OTU_57314 OTU_65907 OTU_1930 OTU_45005 OTU_61616 OTU_72797 OTU_52826 OTU_55115 OTU_62216 OTU_34968\n",
      "Topic #35: OTU_16939 OTU_43680 OTU_68990 OTU_2498 OTU_51727 OTU_3197 OTU_50105 OTU_56047 OTU_4298 OTU_2023 OTU_58621 OTU_52559 OTU_58954 OTU_55865 OTU_31390 OTU_6311 OTU_17783 OTU_27786 OTU_6695 OTU_3145\n",
      "Topic #36: OTU_501 OTU_22874 OTU_60332 OTU_76107 OTU_20106 OTU_40657 OTU_63141 OTU_4237 OTU_6724 OTU_31700 OTU_6763 OTU_70196 OTU_72699 OTU_52843 OTU_74966 OTU_64617 OTU_59039 OTU_49271 OTU_5156 OTU_9423\n",
      "Topic #37: OTU_211 OTU_275 OTU_8658 OTU_16072 OTU_5038 OTU_63779 OTU_19962 OTU_11229 OTU_14830 OTU_2753 OTU_10998 OTU_28554 OTU_31074 OTU_50126 OTU_46256 OTU_72248 OTU_1436 OTU_68332 OTU_17183 OTU_11559\n",
      "Topic #38: OTU_64138 OTU_4589 OTU_16572 OTU_65714 OTU_60901 OTU_684 OTU_59872 OTU_283 OTU_62277 OTU_64331 OTU_58700 OTU_35981 OTU_53950 OTU_10900 OTU_48603 OTU_593 OTU_19396 OTU_11948 OTU_12212 OTU_16225\n",
      "Topic #39: OTU_55770 OTU_56263 OTU_21684 OTU_12133 OTU_49240 OTU_35586 OTU_69895 OTU_11978 OTU_4729 OTU_4161 OTU_71836 OTU_10339 OTU_20317 OTU_2200 OTU_49505 OTU_55847 OTU_67456 OTU_33547 OTU_43320 OTU_37730\n",
      "Topic #40: OTU_1901 OTU_275 OTU_2223 OTU_15492 OTU_8658 OTU_68332 OTU_46682 OTU_55980 OTU_28554 OTU_74083 OTU_14200 OTU_46948 OTU_1979 OTU_75078 OTU_55865 OTU_70860 OTU_3243 OTU_42094 OTU_345 OTU_38151\n",
      "Topic #41: OTU_28625 OTU_41572 OTU_39303 OTU_17405 OTU_41325 OTU_10630 OTU_34826 OTU_58875 OTU_21581 OTU_501 OTU_28977 OTU_5326 OTU_65776 OTU_74677 OTU_75894 OTU_16532 OTU_8658 OTU_46485 OTU_70119 OTU_275\n",
      "Topic #42: OTU_65235 OTU_17862 OTU_37614 OTU_33577 OTU_50099 OTU_275 OTU_16990 OTU_23749 OTU_8658 OTU_19962 OTU_7214 OTU_24716 OTU_49378 OTU_70094 OTU_73931 OTU_14017 OTU_13008 OTU_2396 OTU_28282 OTU_68332\n",
      "Topic #43: OTU_35015 OTU_59752 OTU_59421 OTU_5100 OTU_59490 OTU_24366 OTU_6383 OTU_14899 OTU_19892 OTU_68032 OTU_15365 OTU_69741 OTU_51524 OTU_33163 OTU_2040 OTU_29016 OTU_58228 OTU_24866 OTU_69943 OTU_35362\n",
      "Topic #44: OTU_10733 OTU_66911 OTU_64138 OTU_22952 OTU_75380 OTU_66613 OTU_42572 OTU_72631 OTU_68993 OTU_25509 OTU_25919 OTU_76164 OTU_275 OTU_33620 OTU_53388 OTU_1790 OTU_8658 OTU_7090 OTU_38248 OTU_34014\n",
      "Topic #45: OTU_275 OTU_8658 OTU_2114 OTU_68332 OTU_55980 OTU_28554 OTU_46948 OTU_50392 OTU_60762 OTU_70860 OTU_70 OTU_75078 OTU_1979 OTU_14200 OTU_1436 OTU_3243 OTU_23749 OTU_345 OTU_2147 OTU_75889\n",
      "Topic #46: OTU_7472 OTU_17147 OTU_36348 OTU_61981 OTU_33556 OTU_17408 OTU_76086 OTU_41045 OTU_14838 OTU_60250 OTU_55731 OTU_3438 OTU_68210 OTU_74880 OTU_65872 OTU_70594 OTU_60411 OTU_19419 OTU_64292 OTU_4252\n",
      "Topic #47: OTU_56514 OTU_18184 OTU_18565 OTU_21330 OTU_50697 OTU_275 OTU_5070 OTU_8658 OTU_5544 OTU_14200 OTU_16636 OTU_3036 OTU_68130 OTU_68332 OTU_11188 OTU_345 OTU_28554 OTU_55980 OTU_23749 OTU_45069\n",
      "Topic #48: OTU_275 OTU_8658 OTU_28554 OTU_55980 OTU_68332 OTU_14200 OTU_1436 OTU_75078 OTU_70 OTU_1979 OTU_3243 OTU_2147 OTU_46948 OTU_39309 OTU_23749 OTU_75889 OTU_48910 OTU_54217 OTU_34315 OTU_70860\n",
      "Topic #49: OTU_65829 OTU_4892 OTU_52263 OTU_61103 OTU_27986 OTU_3011 OTU_53970 OTU_68103 OTU_72820 OTU_5818 OTU_17303 OTU_34249 OTU_15412 OTU_8373 OTU_41656 OTU_275 OTU_5746 OTU_6159 OTU_41386 OTU_53546\n",
      "Topic #50: OTU_5697 OTU_881 OTU_275 OTU_69227 OTU_75078 OTU_15576 OTU_6385 OTU_8658 OTU_68332 OTU_472 OTU_28554 OTU_55980 OTU_7532 OTU_437 OTU_56514 OTU_14200 OTU_23668 OTU_501 OTU_1436 OTU_46948\n",
      "Topic #51: OTU_46256 OTU_75782 OTU_28612 OTU_45603 OTU_30666 OTU_10220 OTU_4757 OTU_49527 OTU_15418 OTU_3476 OTU_2993 OTU_4930 OTU_35074 OTU_12198 OTU_1024 OTU_21018 OTU_17506 OTU_25525 OTU_60902 OTU_75230\n",
      "Topic #52: OTU_275 OTU_8658 OTU_55980 OTU_501 OTU_68332 OTU_75078 OTU_14200 OTU_28554 OTU_54217 OTU_345 OTU_47181 OTU_1436 OTU_70860 OTU_26693 OTU_75889 OTU_1979 OTU_46948 OTU_43330 OTU_23749 OTU_22874\n",
      "Topic #53: OTU_23681 OTU_28899 OTU_49064 OTU_34443 OTU_66862 OTU_5797 OTU_8532 OTU_275 OTU_12524 OTU_24679 OTU_55980 OTU_24329 OTU_54264 OTU_8658 OTU_68332 OTU_28554 OTU_73620 OTU_1979 OTU_60076 OTU_60822\n",
      "Topic #54: OTU_2035 OTU_41109 OTU_64834 OTU_6281 OTU_60929 OTU_52939 OTU_26693 OTU_26687 OTU_33111 OTU_18323 OTU_275 OTU_7345 OTU_12353 OTU_43330 OTU_67430 OTU_1675 OTU_74678 OTU_8658 OTU_28554 OTU_44404\n",
      "Topic #55: OTU_50697 OTU_67598 OTU_1788 OTU_275 OTU_8658 OTU_180 OTU_28554 OTU_14200 OTU_55980 OTU_68332 OTU_1979 OTU_70860 OTU_17881 OTU_3243 OTU_345 OTU_75889 OTU_46948 OTU_70 OTU_35881 OTU_1436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #56: OTU_8658 OTU_275 OTU_28554 OTU_501 OTU_55980 OTU_75078 OTU_68332 OTU_1979 OTU_14200 OTU_46948 OTU_75889 OTU_70860 OTU_5380 OTU_70 OTU_46256 OTU_22874 OTU_1436 OTU_54217 OTU_61881 OTU_34315\n",
      "Topic #57: OTU_74471 OTU_43389 OTU_53918 OTU_3706 OTU_56514 OTU_16461 OTU_60761 OTU_53671 OTU_66659 OTU_3623 OTU_62100 OTU_15913 OTU_26018 OTU_21852 OTU_63117 OTU_68072 OTU_2506 OTU_2456 OTU_56157 OTU_18184\n",
      "Topic #58: OTU_275 OTU_28554 OTU_55980 OTU_68332 OTU_8658 OTU_23749 OTU_14200 OTU_75078 OTU_1979 OTU_1436 OTU_2147 OTU_38151 OTU_75889 OTU_46948 OTU_345 OTU_61881 OTU_70860 OTU_54217 OTU_3243 OTU_4775\n",
      "Topic #59: OTU_71232 OTU_60071 OTU_67118 OTU_74070 OTU_72233 OTU_18977 OTU_275 OTU_52528 OTU_42964 OTU_56275 OTU_65379 OTU_1619 OTU_62249 OTU_3847 OTU_10089 OTU_47601 OTU_55980 OTU_60593 OTU_28554 OTU_8658\n",
      "Topic #60: OTU_59664 OTU_75810 OTU_15116 OTU_34306 OTU_5179 OTU_275 OTU_67666 OTU_33350 OTU_7919 OTU_8658 OTU_52200 OTU_70316 OTU_19578 OTU_36766 OTU_55980 OTU_62481 OTU_68332 OTU_44097 OTU_5360 OTU_28554\n",
      "Topic #61: OTU_3475 OTU_56554 OTU_3615 OTU_275 OTU_8658 OTU_572 OTU_14200 OTU_14924 OTU_55980 OTU_68332 OTU_28554 OTU_75889 OTU_345 OTU_46948 OTU_1436 OTU_1979 OTU_23749 OTU_54217 OTU_75078 OTU_41183\n",
      "Topic #62: OTU_3084 OTU_46051 OTU_8406 OTU_32883 OTU_501 OTU_22323 OTU_1290 OTU_53037 OTU_61337 OTU_15474 OTU_38396 OTU_8100 OTU_55968 OTU_39346 OTU_5051 OTU_2454 OTU_275 OTU_30140 OTU_37849 OTU_43521\n",
      "Topic #63: OTU_37847 OTU_606 OTU_75158 OTU_70849 OTU_57302 OTU_32911 OTU_27477 OTU_31409 OTU_74704 OTU_73438 OTU_58532 OTU_9813 OTU_50230 OTU_49978 OTU_62541 OTU_44404 OTU_46256 OTU_73109 OTU_58890 OTU_65363\n",
      "Topic #64: OTU_275 OTU_28554 OTU_55980 OTU_8658 OTU_68332 OTU_45601 OTU_46948 OTU_345 OTU_75078 OTU_3243 OTU_1979 OTU_23749 OTU_1436 OTU_70860 OTU_54217 OTU_4775 OTU_14200 OTU_38151 OTU_47181 OTU_70\n",
      "Topic #65: OTU_275 OTU_8658 OTU_28554 OTU_46948 OTU_55980 OTU_1979 OTU_75889 OTU_75078 OTU_14200 OTU_68332 OTU_1436 OTU_4775 OTU_54217 OTU_70 OTU_2147 OTU_23749 OTU_345 OTU_45601 OTU_47181 OTU_38151\n",
      "Topic #66: OTU_68313 OTU_275 OTU_73359 OTU_8658 OTU_28554 OTU_1436 OTU_55980 OTU_21478 OTU_68332 OTU_3243 OTU_14200 OTU_2114 OTU_46948 OTU_54217 OTU_4775 OTU_52540 OTU_345 OTU_70 OTU_23749 OTU_1979\n",
      "Topic #67: OTU_56514 OTU_18184 OTU_18565 OTU_21330 OTU_5070 OTU_5544 OTU_16636 OTU_68130 OTU_3036 OTU_45069 OTU_11188 OTU_59419 OTU_23809 OTU_71279 OTU_15089 OTU_51188 OTU_32165 OTU_5312 OTU_41077 OTU_20889\n",
      "Topic #68: OTU_36307 OTU_75077 OTU_23247 OTU_61713 OTU_148 OTU_3510 OTU_53587 OTU_3575 OTU_33697 OTU_3227 OTU_75973 OTU_275 OTU_7418 OTU_68507 OTU_64423 OTU_46866 OTU_50503 OTU_76055 OTU_8193 OTU_3540\n",
      "Topic #69: OTU_52296 OTU_275 OTU_8658 OTU_55980 OTU_68332 OTU_45465 OTU_28554 OTU_1436 OTU_14200 OTU_45601 OTU_62930 OTU_70860 OTU_3243 OTU_2147 OTU_23749 OTU_1979 OTU_75889 OTU_70 OTU_47181 OTU_75078\n",
      "Topic #70: OTU_46256 OTU_75782 OTU_45603 OTU_28612 OTU_10220 OTU_30666 OTU_4757 OTU_2993 OTU_15418 OTU_35074 OTU_275 OTU_3476 OTU_43116 OTU_12198 OTU_25525 OTU_1024 OTU_17506 OTU_55980 OTU_8658 OTU_75230\n",
      "Topic #71: OTU_6281 OTU_54839 OTU_48677 OTU_2569 OTU_69593 OTU_76136 OTU_35749 OTU_38750 OTU_54747 OTU_43888 OTU_56071 OTU_11137 OTU_43177 OTU_29026 OTU_8334 OTU_41257 OTU_34433 OTU_43107 OTU_15293 OTU_60817\n",
      "Topic #72: OTU_275 OTU_8658 OTU_55980 OTU_28554 OTU_1436 OTU_68332 OTU_3949 OTU_2147 OTU_6171 OTU_14200 OTU_75889 OTU_1979 OTU_64138 OTU_3243 OTU_46948 OTU_23749 OTU_39132 OTU_54217 OTU_2753 OTU_47181\n",
      "Topic #73: OTU_275 OTU_8658 OTU_28554 OTU_55980 OTU_68332 OTU_14200 OTU_1436 OTU_46948 OTU_1979 OTU_75078 OTU_345 OTU_3243 OTU_23749 OTU_70 OTU_75889 OTU_70860 OTU_2147 OTU_39309 OTU_54217 OTU_47181\n",
      "Topic #74: OTU_46256 OTU_275 OTU_8658 OTU_68332 OTU_28554 OTU_1436 OTU_55980 OTU_46948 OTU_75782 OTU_14200 OTU_1979 OTU_75889 OTU_75078 OTU_39309 OTU_54217 OTU_45601 OTU_23749 OTU_345 OTU_70860 OTU_70\n",
      "Topic #75: OTU_39702 OTU_64858 OTU_275 OTU_49104 OTU_15635 OTU_8658 OTU_55325 OTU_10512 OTU_75864 OTU_68332 OTU_42712 OTU_28554 OTU_55980 OTU_3461 OTU_52690 OTU_1436 OTU_345 OTU_6590 OTU_71398 OTU_75889\n",
      "Topic #76: OTU_275 OTU_8658 OTU_68332 OTU_55980 OTU_28554 OTU_1436 OTU_75889 OTU_1979 OTU_345 OTU_54217 OTU_2147 OTU_46948 OTU_14200 OTU_75078 OTU_3243 OTU_4775 OTU_23749 OTU_70860 OTU_38151 OTU_3305\n",
      "Topic #77: OTU_501 OTU_275 OTU_22874 OTU_8658 OTU_28554 OTU_59792 OTU_55980 OTU_31666 OTU_68332 OTU_20106 OTU_60332 OTU_40657 OTU_75078 OTU_76107 OTU_70 OTU_3243 OTU_63638 OTU_345 OTU_1979 OTU_70860\n",
      "Topic #78: OTU_17517 OTU_6934 OTU_73058 OTU_48916 OTU_13126 OTU_60818 OTU_42525 OTU_74090 OTU_58923 OTU_73409 OTU_34311 OTU_20363 OTU_47175 OTU_60371 OTU_46907 OTU_6165 OTU_57727 OTU_30779 OTU_57527 OTU_72804\n",
      "Topic #79: OTU_54278 OTU_12275 OTU_65287 OTU_38906 OTU_31553 OTU_6171 OTU_3949 OTU_8634 OTU_40752 OTU_43769 OTU_44895 OTU_44066 OTU_74841 OTU_5691 OTU_31971 OTU_46256 OTU_2503 OTU_39022 OTU_46315 OTU_4287\n",
      "Topic #80: OTU_43091 OTU_59569 OTU_30179 OTU_58520 OTU_19059 OTU_6406 OTU_16419 OTU_46256 OTU_44411 OTU_35015 OTU_70330 OTU_22991 OTU_16661 OTU_24465 OTU_75774 OTU_12007 OTU_31031 OTU_34025 OTU_48879 OTU_4250\n",
      "Topic #81: OTU_606 OTU_75158 OTU_57302 OTU_70849 OTU_32911 OTU_27477 OTU_50230 OTU_62541 OTU_73438 OTU_9813 OTU_65363 OTU_1976 OTU_2809 OTU_28612 OTU_50241 OTU_73029 OTU_75575 OTU_21757 OTU_72138 OTU_42897\n",
      "Topic #82: OTU_18943 OTU_7406 OTU_75683 OTU_55726 OTU_23937 OTU_4823 OTU_964 OTU_64014 OTU_67429 OTU_4211 OTU_14201 OTU_5825 OTU_37659 OTU_61282 OTU_3255 OTU_54584 OTU_51733 OTU_20169 OTU_8261 OTU_65372\n",
      "Topic #83: OTU_275 OTU_8658 OTU_64138 OTU_28554 OTU_68332 OTU_55980 OTU_345 OTU_1979 OTU_70 OTU_75078 OTU_1436 OTU_14200 OTU_70860 OTU_75889 OTU_39309 OTU_47181 OTU_50697 OTU_3243 OTU_2147 OTU_46948\n",
      "Topic #84: OTU_275 OTU_1452 OTU_8658 OTU_68332 OTU_13603 OTU_28554 OTU_1436 OTU_17489 OTU_14200 OTU_345 OTU_75078 OTU_55980 OTU_1979 OTU_2147 OTU_41312 OTU_75889 OTU_54217 OTU_23749 OTU_16721 OTU_38151\n",
      "Topic #85: OTU_50091 OTU_41912 OTU_994 OTU_49065 OTU_62081 OTU_14877 OTU_68333 OTU_71207 OTU_59580 OTU_65055 OTU_13962 OTU_46256 OTU_75503 OTU_1347 OTU_31634 OTU_66716 OTU_59711 OTU_8431 OTU_11049 OTU_19426\n",
      "Topic #86: OTU_68313 OTU_73359 OTU_21478 OTU_63028 OTU_52540 OTU_61977 OTU_22728 OTU_65319 OTU_3046 OTU_44501 OTU_55008 OTU_30766 OTU_28627 OTU_5287 OTU_7174 OTU_2944 OTU_57006 OTU_50804 OTU_43511 OTU_71961\n",
      "Topic #87: OTU_275 OTU_8658 OTU_28554 OTU_55980 OTU_68332 OTU_14200 OTU_1436 OTU_75078 OTU_345 OTU_45601 OTU_75889 OTU_1979 OTU_47181 OTU_70860 OTU_17517 OTU_3243 OTU_75774 OTU_46948 OTU_39132 OTU_38151\n",
      "Topic #88: OTU_1626 OTU_69227 OTU_15576 OTU_7223 OTU_53313 OTU_75078 OTU_1456 OTU_53924 OTU_25207 OTU_104 OTU_1693 OTU_64378 OTU_36527 OTU_28626 OTU_29119 OTU_704 OTU_69827 OTU_39420 OTU_5425 OTU_17399\n",
      "Topic #89: OTU_46256 OTU_275 OTU_8658 OTU_75782 OTU_55980 OTU_68332 OTU_75078 OTU_14200 OTU_46948 OTU_1436 OTU_345 OTU_28554 OTU_1979 OTU_54217 OTU_70 OTU_38151 OTU_3169 OTU_2147 OTU_70860 OTU_45601\n",
      "Topic #90: OTU_43330 OTU_26693 OTU_74678 OTU_74956 OTU_2599 OTU_65690 OTU_1482 OTU_63845 OTU_60180 OTU_43888 OTU_4220 OTU_64331 OTU_22602 OTU_72717 OTU_1874 OTU_67919 OTU_58141 OTU_5153 OTU_55800 OTU_7120\n",
      "Topic #91: OTU_3949 OTU_6171 OTU_57152 OTU_16620 OTU_17473 OTU_60131 OTU_18049 OTU_23320 OTU_66970 OTU_57793 OTU_74880 OTU_61064 OTU_45663 OTU_23607 OTU_12890 OTU_75650 OTU_4938 OTU_3578 OTU_40673 OTU_63838\n",
      "Topic #92: OTU_275 OTU_8658 OTU_68332 OTU_28554 OTU_55980 OTU_75078 OTU_14200 OTU_1979 OTU_3243 OTU_75889 OTU_23749 OTU_46948 OTU_4775 OTU_61881 OTU_70 OTU_54217 OTU_345 OTU_70860 OTU_2147 OTU_34315\n",
      "Topic #93: OTU_44404 OTU_16637 OTU_56956 OTU_54020 OTU_3068 OTU_35093 OTU_66167 OTU_1669 OTU_75352 OTU_71806 OTU_39839 OTU_24761 OTU_66086 OTU_55602 OTU_65797 OTU_51105 OTU_73763 OTU_58388 OTU_8294 OTU_66085\n",
      "Topic #94: OTU_275 OTU_8658 OTU_28554 OTU_68332 OTU_55980 OTU_14200 OTU_1979 OTU_75078 OTU_1436 OTU_345 OTU_46948 OTU_70 OTU_54217 OTU_3243 OTU_2147 OTU_70860 OTU_30966 OTU_38151 OTU_75889 OTU_47181\n",
      "Topic #95: OTU_275 OTU_28554 OTU_69410 OTU_45967 OTU_68332 OTU_8658 OTU_55980 OTU_606 OTU_1979 OTU_1436 OTU_70 OTU_345 OTU_54217 OTU_14200 OTU_75078 OTU_46948 OTU_75889 OTU_3243 OTU_23749 OTU_47181\n",
      "Topic #96: OTU_275 OTU_8658 OTU_28554 OTU_1436 OTU_55980 OTU_46948 OTU_75078 OTU_14200 OTU_68332 OTU_70 OTU_1979 OTU_54217 OTU_345 OTU_39309 OTU_38151 OTU_75889 OTU_23749 OTU_2147 OTU_45601 OTU_34315\n",
      "Topic #97: OTU_5380 OTU_58089 OTU_5885 OTU_3158 OTU_617 OTU_57003 OTU_275 OTU_31674 OTU_14018 OTU_8658 OTU_67258 OTU_61637 OTU_68332 OTU_51742 OTU_28130 OTU_2944 OTU_61666 OTU_71505 OTU_75078 OTU_42877\n",
      "Topic #98: OTU_12726 OTU_43527 OTU_932 OTU_4505 OTU_36419 OTU_74448 OTU_33912 OTU_10516 OTU_7671 OTU_5601 OTU_827 OTU_41774 OTU_24174 OTU_28369 OTU_47386 OTU_67078 OTU_51947 OTU_3719 OTU_19803 OTU_73189\n",
      "Topic #99: OTU_57545 OTU_52625 OTU_2145 OTU_56906 OTU_6365 OTU_65017 OTU_64904 OTU_49120 OTU_6224 OTU_53509 OTU_74040 OTU_1023 OTU_2572 OTU_49454 OTU_71981 OTU_53766 OTU_57806 OTU_27747 OTU_36858 OTU_12057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_vectorizer._validate_vocabulary()\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "#print('tf_vectorizer.get_feature_names(): {0}'.\n",
    "#  format(tf_vectorizer.get_feature_names()))\n",
    "\n",
    "n_top_words = 20\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next steps: validate (scatter plots of 'words' and 'topics'); test different parameters; scale up to larger sample size. Can also try NMF (but will need to do tf-idf transform).\n",
    "\n",
    "To map taxa to geographic locations, need to map them to the samples that they are present in. The samples are then associated with geog coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
